{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9c950075-6a9e-4709-a609-77d0b6762333",
      "metadata": {
        "id": "9c950075-6a9e-4709-a609-77d0b6762333",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# NDIF Main Demo Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97292f90-5c2e-444d-88e4-41265e8ad0d9",
      "metadata": {
        "id": "97292f90-5c2e-444d-88e4-41265e8ad0d9",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4f9b4224",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Install necessary libraries (don't do this in cell, because you need ipykernel first anyway)\n",
        "# %pip install -r requirements.txt\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import gdown\n",
        "import zipfile\n",
        "from IPython.display import clear_output\n",
        "\n",
        "root = Path(\"/root/function_vectors\")\n",
        "assert root.exists()\n",
        "\n",
        "if not (root / \"ndif-dev\").exists():\n",
        "\n",
        "    file_id = \"1jS0ydba19uPXCC786_Sx1Bylqx-p0hEc\"\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}&export=download\"\n",
        "    output = \"ndif-dev.zip\"\n",
        "\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "    with zipfile.ZipFile(output, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(root)\n",
        "\n",
        "    os.remove(output)\n",
        "\n",
        "if 'ndif-dev' not in sys.path:\n",
        "    sys.path.append('ndif-dev')\n",
        "    # %pip install -e ndif-dev\n",
        "\n",
        "import engine\n",
        "from engine import LanguageModel\n",
        "from engine.intervention import InterventionProxy\n",
        "\n",
        "from IPython import get_ipython\n",
        "ipython = get_ipython()\n",
        "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
        "ipython.run_line_magic(\"autoreload\", \"2\")\n",
        "\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9582790f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import einops\n",
        "import circuitsvis as cv\n",
        "import plotly.express as px\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from jaxtyping import Int, Float\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display\n",
        "import transformer_lens.utils as utils\n",
        "import webbrowser\n",
        "from rich import print as rprint\n",
        "from rich.table import Table\n",
        "import openai\n",
        "import time\n",
        "import gc\n",
        "import string\n",
        "\n",
        "from plotly_utils import imshow, line\n",
        "\n",
        "import tests\n",
        "# import function_vectors.tests as tests\n",
        "\n",
        "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "assert str(device) == \"cuda\"\n",
        "\n",
        "t.set_grad_enabled(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1b0b1aa1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# gpt2 = LanguageModel('gpt2', device_map=device)\n",
        "model = LanguageModel('EleutherAI/gpt-j-6b', device_map=device)\n",
        "tokenizer = model.tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fae35f9e",
      "metadata": {},
      "source": [
        "# 1️⃣ Introduction to `nnsight`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c43466c5",
      "metadata": {},
      "source": [
        "## Important syntax\n",
        "\n",
        "Here, we'll discuss some important syntax for interacting with `nnsight` models. Since these models are extensions of HuggingFace models, some of this information (e.g. tokenization) applies to plain HuggingFace models as well as `nnsight` models, and some of it (e.g. forward passes) is specific to `nnsight`, i.e. it would work differently if you just had a standard HuggingFace model. Before each section, we'll indicate which is which.\n",
        "\n",
        "### Model config\n",
        "\n",
        "*This applies to HuggingFace and `nnsight` models.*\n",
        "\n",
        "Each model comes with a `model.config`, which contains lots of useful information about the model (e.g. number of heads and layers, size of hidden layers, etc.). You can access this with `model.config`. Run the code below to see this in action, and to define some useful variables for later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "22ac7699",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of heads: 16\n",
            "Number of layers: 28\n",
            "Model dimension: 4096\n",
            "Head dimension: 256\n",
            "\n",
            "GPTJConfig {\n",
            "  \"_name_or_path\": \"EleutherAI/gpt-j-6b\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPTJForCausalLM\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.0,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gptj\",\n",
            "  \"n_embd\": 4096,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 28,\n",
            "  \"n_positions\": 2048,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rotary\": true,\n",
            "  \"rotary_dim\": 64,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50,\n",
            "      \"temperature\": 1.0\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
            "  \"transformers_version\": \"4.33.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50400\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "N_HEADS = model.config.n_head\n",
        "N_LAYERS = model.config.n_layer\n",
        "D_MODEL = model.config.n_embd\n",
        "D_HEAD = D_MODEL // N_HEADS\n",
        "\n",
        "print(f\"Number of heads: {N_HEADS}\")\n",
        "print(f\"Number of layers: {N_LAYERS}\")\n",
        "print(f\"Model dimension: {D_MODEL}\")\n",
        "print(f\"Head dimension: {D_HEAD}\\n\")\n",
        "\n",
        "print(model.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6e143e",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Tokenizers\n",
        "\n",
        "*This applies to HuggingFace and `nnsight` models.*\n",
        "\n",
        "A model comes with a tokenizer, accessable with `model.tokenizer` (just like TransformerLens). Unlike TransformerLens, we won't be using utility functions like `model.to_str_toks`, instead we'll be using the tokenizer directly. Some important functions for today's exercises are:\n",
        "\n",
        "* `tokenizer` (i.e. just calling it on some input)\n",
        "    * This takes in a string (or list of strings) and returns the tokenized version.\n",
        "    * It will return a dictionary, always containing `input_ids` (i.e. the actual tokens) but also other things which are specific to the transformer model (e.g. `attention_mask` - see dropdown).\n",
        "    * Other useful arguments for this function:\n",
        "        * `return_tensors` - if this is `\"pt\"`, you'll get results returned as PyTorch tensors, rather than lists (which is the default).\n",
        "        * `padding` - if True (default is False), the tokenizer can accept sequences of variable length. The shorter sequences get padded at the beginning (see dropdown below for more).\n",
        "* `tokenizer.decode`\n",
        "    * This takes in tokens, and returns the decoded string.\n",
        "    * If the input is an integer, it returns the corresponding string. If the input is a list / 1D array of integers, it returns all those strings concatenated (which can sometimes not be what you want).\n",
        "* `tokenizer.batch_decode`\n",
        "    * Equivalent to `tokenizer.decode`, but it doesn't concatenate.\n",
        "    * If the input is a list / 1D integer array, it returns a list of strings. If the input is 2D, it will concatenate within each list.\n",
        "* `tokenizer.tokenize`\n",
        "    * Takes in a string, and returns a list of strings.\n",
        "\n",
        "Run the code below to see some examples of these functions in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "602cc9db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[1212, 1276,  307, 3635]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
            "I never could get the hang of Thursdays.\n",
            "['These', ' words', ' will', ' be', ' split', ' up']\n",
            "['This sentence will be together', 'So will this one']\n",
            "['This', 'Ġsentence', 'Ġwill', 'Ġbe', 'Ġtoken', 'ized']\n"
          ]
        }
      ],
      "source": [
        "# Calling tokenizer returns a dictionary, containing input ids & other data.\n",
        "# If returned as a tensor, then by default it will have a batch dimension.\n",
        "print(tokenizer(\"This must be Thursday\", return_tensors=\"pt\"))\n",
        "\n",
        "# Decoding a list of integers, into a concatenated string.\n",
        "print(tokenizer.decode([40, 1239, 714, 651, 262, 8181, 286, 48971, 12545, 13]))\n",
        "\n",
        "# Using batch decode, on both 1D and 2D input.\n",
        "print(tokenizer.batch_decode([4711, 2456, 481, 307, 6626, 510]))\n",
        "print(tokenizer.batch_decode([[1212, 6827, 481, 307, 1978], [2396, 481, 428, 530]]))\n",
        "\n",
        "# Split sentence into tokens (note we see the special Ġ character in place of prepended spaces).\n",
        "print(tokenizer.tokenize(\"This sentence will be tokenized\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c800352",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Note on <code>attention_mask</code> (optional)</summary>\n",
        "\n",
        "`attention_mask`, which is a series of 1s and 0s. We mask attention at all 0-positions (i.e. we don't allow these tokens to be attended to). This is useful when you have to do padding. For example:\n",
        "\n",
        "```python\n",
        "model.tokenizer([\"Hello world\", \"Hello\"], return_tensors=\"pt\", padding=True)\n",
        "```\n",
        "\n",
        "will return:\n",
        "\n",
        "```\n",
        "{\n",
        "    'attention_mask': tensor([[1, 1], [0, 1]]),\n",
        "    'input_ids': tensor([[15496,   995], [50256, 15496]])\n",
        "}\n",
        "```\n",
        "\n",
        "We can see how the shorter sequence has been padded at the beginning, and attention to this token will be masked.\n",
        "\n",
        "</details>\n",
        "\n",
        "### Model outputs\n",
        "\n",
        "*This applies to HuggingFace and `nnsight` models.*\n",
        "\n",
        "If you've worked with TransformerLens, then you'll be used to thinking of logits as the default output of a model, when you run a forward pass on that model.\n",
        "\n",
        "HuggingFace models are a bit different. The standard way to get output from them is using the `model.generate` method. This method takes in a dictionary of inputs (which you can get from the tokenizer), and returns an object which contains a bunch of different things: the actual tokens generated by the model, plus maybe a few other things depending on what arguments you passed to `generate` (e.g. this might include logits, or hidden states).\n",
        "\n",
        "The `nnsight` models we'll be using here are based on HuggingFace models, and we'll also be using `model.generate` which takes basically the same arguments, and produces an output object that contains the same kind of information. However, the exact way we use this method is quite different for `nnsight`...\n",
        "\n",
        "### Running the model\n",
        "\n",
        "*This only applies to `nnsight` models.*\n",
        "\n",
        "Rather than just calling `model.generate`, we use a **context manager** to run the model. This is useful because we can access & do things with the internal state of the model, in the middle of the forward pass. Using this context manager is like setting up a set of detailed instructions for how the forward pass will work, and only when you exit the context manager are the instructions actually sent off & executed.\n",
        "\n",
        "Below is the simplest example of code to run the model (and also access the internal states of the model). Run it and look at the output, then read the explanation below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dd35e1e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Running this code so we don't get this printout each time we run a fwd pass\n",
        "warnings.filterwarnings(\"ignore\", message=\"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1e23cdad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['The Eiffel Tower is in the city of Paris,']\n",
            "Residual stream shape =  torch.Size([1, 10, 4096])\n"
          ]
        }
      ],
      "source": [
        "prompt = 'The Eiffel Tower is in the city of'\n",
        "\n",
        "with model.generate(max_new_tokens=2, pad_token_id=tokenizer.eos_token_id) as generator:\n",
        "    with generator.invoke(prompt) as invoker:\n",
        "        i = model.transformer.h[-1].input.save()\n",
        "        hidden_states: InterventionProxy = model.transformer.h[-1].output[0].save()\n",
        "\n",
        "# Get output, which is a tensor of token IDs, of shape (1, seq_len+1)\n",
        "output = generator.output\n",
        "print([model.tokenizer.decode(t) for t in output])\n",
        "\n",
        "# Get hidden states, which are the value of the residual stream at last layer\n",
        "print(\"Residual stream shape = \", hidden_states.value.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28a06fca-66e8-4c42-abe1-0e3c09d65c27",
      "metadata": {
        "id": "28a06fca-66e8-4c42-abe1-0e3c09d65c27"
      },
      "source": [
        "Lets go over this piece by piece.\n",
        "\n",
        "**First, we create a generation context block** by calling `.generate(...)` on the model object. This denotes that we wish to generate tokens given some prompts.\n",
        "\n",
        "```python\n",
        "with model.generate(max_new_tokens=1, pad_token_id=tokenizer.eos_token_id) as generator:\n",
        "```\n",
        "\n",
        "Calling `.generate(...)` does not actually initialize or run the model. Only after the `with ... as generator:` block is exited is the model actually loaded and run. All operations in the block are \"proxies\" which essentially creates a graph of operations we wish to carry out later.\n",
        "\n",
        "The `max_new_tokens=1` argument just means we do a single forward pass, rather than autoregressively generate multiple tokens. The `pad_token_id` argument isn't strictly necessary (this is the default behaviour anyway), it just suppresses a warning message that would otherwise be printed.\n",
        "\n",
        "**Within the generation context,** we create invocation contexts to specify the actual prompts we want to run.\n",
        "\n",
        "```python\n",
        "with generator.invoke(PROMPT) as invoker:\n",
        "```\n",
        "\n",
        "**Within an invoke context**, all operations/interventions will be applied to the processing of the prompt. Models can be run on a variety of input formats: strings, lists of tokens, tensors of tokens, etc.\n",
        "\n",
        "This is all we actually need to run a forward pass on the model. We could replace the `hidden_states` line with just `pass`, and we'd still be able to access the model output in the same way. But the most interesting part of `nnsight` is the ability to access the model's internal states (like you've probably already done with TransformerLens). Let's see how this works!\n",
        "\n",
        "```\n",
        "hidden_states = model.transformer.h[-1].output[0].save()\n",
        "```\n",
        "\n",
        "On this line we're saying: access the last layer of the transformer `model.transformer.h[-1]`, access this layer's output `.output` (which is a tuple of tensors), index the first tensor in this tuple `.output[0]`, and save it `.save()`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4024bac0",
      "metadata": {},
      "source": [
        "Let's break down this line in a bit more detail:\n",
        "\n",
        "#### `model.transformer.h[-1]`\n",
        "\n",
        "If you print out the model's architecture with `print(model)`, you'll see that it consists of `transformer` and `lm_head` (for \"language modelling head\"). The `transformer` module is made up of embeddings & dropout, a series of layers (called `.h`, for \"hidden states\"), and a final layernorm.\n",
        "\n",
        "When you're working with different model architectures, it'll often be necessary to print out the model / visit the source code page, to see exactly how they work and what different modules are named. [Here](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) is the source code page for GPT-J.\n",
        "\n",
        "#### `.output[0]`\n",
        "\n",
        "When you access `.output` of a module within a context manager, you're returning a **proxy** for the output of this module during inference. Doing operations on it (like indexing it) also return proxies.\n",
        "\n",
        "Note, modules often have output (and input) stored in tuples. Even if there is only one output and one input, these can be stored as length-1 tuples. In this particular case, `model.transformer.h[-1]` is a `GPTJBlock` module, which outputs a length-2 tensor. The 0th is the residual stream at the end of the block (i.e. the thing we want in this case).\n",
        "\n",
        "<details>\n",
        "<summary>Optional exercise - from the <a href=\"https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html\">source code page</a>, can you figure out what the second output in this tuple is?</summary>\n",
        "\n",
        "The second output is also a tuple of tensors, of length 2. In the GPT-J source code, they are called `present`. They represent the keys and values which were calculated in this forward pass (as opposed to those that were calculated in an earlier forward pass, and cached by the model). Since we're only generating one new token, these are just the full keys and values.\n",
        "\n",
        "</details>\n",
        "\n",
        "When debugging, you can call `.shape` on a proxy. This will even work if the proxy represents a tuple of tensors; you'll get a tuple of all the sizes of these tensors.\n",
        "\n",
        "You can also use `.input` to access the inputs to a module - this works in the same way (often also stored as a tuple).\n",
        "\n",
        "#### `.save()`\n",
        "\n",
        "This informs the computation graph to clone the value of a proxy, allowing us to access the value of a proxy after generation.\n",
        "\n",
        "During processing of the intervention computational graph we are building, when the value of a proxy is no longer ever needed, its value is dereferenced and destroyed. If you've saved, then you'll be able to access the value of the proxy after this happens (i.e. outside the context manager), using the `.value` attribute."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2f3a06",
      "metadata": {},
      "source": [
        "### Exercise - visualize attention heads\n",
        "\n",
        "```c\n",
        "Difficulty: 🔴🔴⚪⚪⚪\n",
        "Importance: 🔵🔵🔵⚪⚪\n",
        "\n",
        "You should spend up to 10-20 minutes on this exercise.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49cb25d0",
      "metadata": {},
      "source": [
        "That was a lot, so lets put it into practice. Your first task is to extract the attention patterns from the zeroth layer of the transformer, and visualize them using circuitsvis. As a reminder, the syntax for circuitsvis is:\n",
        "\n",
        "```python\n",
        "cv.attention.attention_patterns(\n",
        "    tokens=tokens,\n",
        "    attention=attention,\n",
        ")\n",
        "```\n",
        "\n",
        "where `tokens` is a list of strings, and `attention` is a tensor of shape `(num_heads, num_tokens, num_tokens)`.\n",
        "\n",
        "If you're stuck, [here's a link](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) to the source code for GPT-J. Look for how the attention patterns are calculated, within the `GPTJAttention` block.\n",
        "\n",
        "*Note - this model uses dropout on the attention probabilities, as you'll probably notice from looking at the source code in the link above. This won't affect the model's behaviour because dropout is disabled in inference mode (and using the `generate` method always puts a model in inference mode). But it is still a layer which exists in the model, so you can access its input or output just like any other module.*\n",
        "\n",
        "<details>\n",
        "<summary>Aside - inference mode</summary>\n",
        "\n",
        "Dropout is one of the two main layers whose behaviour changes in inference mode (the other is BatchNorm).\n",
        "\n",
        "If you want to run the model without inference mode, you can wrap your code in `with model.forward(inference=False):`. However, you don't need to worry about this for the purposes of these exercises.\n",
        "\n",
        "</details>\n",
        "\n",
        "If you're stuck indexing the model, see the following hint:\n",
        "\n",
        "<details>\n",
        "<summary>Hint - what module you should get attention from</summary>\n",
        "\n",
        "You want to extract attention from `model.transformer.h[0].attn.attn_dropout.input`. If you used `.output`, it would give you the same values (although they might differ by a dummy batch dimension). Both of these will return a single tensor, because dropout layers take just one input and return just one output.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "48a50787",
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.generate(max_new_tokens=1, pad_token_id=tokenizer.eos_token_id) as generator:\n",
        "    with generator.invoke(prompt) as invoker:\n",
        "        attn_patterns = model.transformer.h[0].attn.attn_dropout.input.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f9be3c74",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0 Head Attention Patterns:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div id=\"circuits-vis-a9aaae2a-e0e8\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-a9aaae2a-e0e8\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"The\", \" E\", \"iff\", \"el\", \" Tower\", \" is\", \" in\", \" the\", \" city\", \" of\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.829063355922699, 0.1709366738796234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10449792444705963, 0.8631790280342102, 0.03232300281524658, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002194455824792385, 0.01044239941984415, 0.9758203029632568, 0.011542831547558308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01957514137029648, 0.0005238918820396066, 0.8991208672523499, 0.07888323813676834, 0.0018968796357512474, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09968297183513641, 0.002919913735240698, 0.0023292850237339735, 0.008620481006801128, 0.11867374181747437, 0.7677736282348633, 0.0, 0.0, 0.0, 0.0], [0.1369725465774536, 0.03587929904460907, 0.009825349785387516, 0.013134374283254147, 0.036710482090711594, 0.520531415939331, 0.24694649875164032, 0.0, 0.0, 0.0], [0.09312178194522858, 0.008496217429637909, 0.0036471723578870296, 0.006210173014551401, 0.007328539155423641, 0.2238079458475113, 0.0488654300570488, 0.6085227727890015, 0.0, 0.0], [0.022006427869200706, 0.005823437124490738, 0.0057095373049378395, 0.003698136657476425, 0.05718507990241051, 0.18564170598983765, 0.06623344123363495, 0.5254097580909729, 0.12829247117042542, 0.0], [0.004313651472330093, 0.000997802708297968, 0.00022000973694957793, 0.0013666593004018068, 0.005537914112210274, 0.03576671704649925, 0.00845794752240181, 0.06083754450082779, 0.7149895429611206, 0.1675121784210205]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7337515354156494, 0.2662484645843506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01838100515305996, 0.9661518931388855, 0.015467054210603237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28406479954719543, 0.4624061584472656, 0.08217735588550568, 0.17135171592235565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29254311323165894, 0.025398923084139824, 0.23610815405845642, 0.11693140864372253, 0.3290184438228607, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2333838790655136, 0.06438977271318436, 0.054571252316236496, 0.04971354082226753, 0.4808805286884308, 0.11706104129552841, 0.0, 0.0, 0.0, 0.0], [0.1657487154006958, 0.036285143345594406, 0.039978135377168655, 0.03913314640522003, 0.4151957333087921, 0.12842589616775513, 0.17523330450057983, 0.0, 0.0, 0.0], [0.18088535964488983, 0.027216654270887375, 0.0364697128534317, 0.025995688512921333, 0.1539277285337448, 0.19731678068637848, 0.16624368727207184, 0.2119443267583847, 0.0, 0.0], [0.061875078827142715, 0.01634238287806511, 0.053551092743873596, 0.02586761675775051, 0.3887181580066681, 0.02492697909474373, 0.07002407312393188, 0.07096059620380402, 0.2877340316772461, 0.0], [0.1078551709651947, 0.010135271586477757, 0.03009823150932789, 0.015300117433071136, 0.13075672090053558, 0.06642581522464752, 0.08435479551553726, 0.1750207394361496, 0.22528469562530518, 0.1547684371471405]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9375947713851929, 0.06240519881248474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5738810300827026, 0.3882512152194977, 0.037867821753025055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6073166728019714, 0.1819608360528946, 0.10958211869001389, 0.10114036500453949, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2613397538661957, 0.4545661211013794, 0.04196810722351074, 0.10260388255119324, 0.13952212035655975, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7512133717536926, 0.030134251341223717, 0.03863786906003952, 0.03444979339838028, 0.05264578387141228, 0.09291895478963852, 0.0, 0.0, 0.0, 0.0], [0.08130466192960739, 0.008510569110512733, 0.011866848915815353, 0.0038354594726115465, 0.047947682440280914, 0.13581959903240204, 0.7107151746749878, 0.0, 0.0, 0.0], [0.16110482811927795, 0.005045980680733919, 0.00649917172268033, 0.006183331366628408, 0.004202307667583227, 0.006440227385610342, 0.09915072470903397, 0.7113734483718872, 0.0, 0.0], [0.023170091211795807, 0.00899879913777113, 0.03188159689307213, 0.010966754518449306, 0.7636092901229858, 0.008237296715378761, 0.07699004560709, 0.033955082297325134, 0.042191095650196075, 0.0], [0.116624616086483, 0.004521339666098356, 0.005119765643030405, 0.006424614693969488, 0.010177543386816978, 0.008108649402856827, 0.19665247201919556, 0.48110663890838623, 0.012579265050590038, 0.15868504345417023]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873823523521423, 0.012617615982890129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.705640971660614, 0.03949311003088951, 0.2548658847808838, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5494459271430969, 0.027415193617343903, 0.19888116419315338, 0.22425773739814758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13006022572517395, 0.023235522210597992, 0.37218791246414185, 0.35988831520080566, 0.11462795734405518, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1305249035358429, 0.021449685096740723, 0.12894372642040253, 0.27416539192199707, 0.057148270308971405, 0.38776805996894836, 0.0, 0.0, 0.0, 0.0], [0.25049659609794617, 0.007087950129061937, 0.09113315492868423, 0.1751839518547058, 0.07337385416030884, 0.13667626678943634, 0.2660481929779053, 0.0, 0.0, 0.0], [0.11081762611865997, 0.010958189144730568, 0.04083585739135742, 0.06034961715340614, 0.032030925154685974, 0.09623461961746216, 0.19093474745750427, 0.4578384757041931, 0.0, 0.0], [0.05363744869828224, 0.025838658213615417, 0.09583628177642822, 0.18366430699825287, 0.05054944008588791, 0.133483424782753, 0.23405790328979492, 0.17247045040130615, 0.05046214163303375, 0.0], [0.0948793813586235, 0.015751123428344727, 0.05621325969696045, 0.10723130404949188, 0.039503198117017746, 0.10456932336091995, 0.09332013130187988, 0.25014761090278625, 0.06267370283603668, 0.1757110208272934]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8586137890815735, 0.14138615131378174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5009570717811584, 0.17692223191261292, 0.32212066650390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01860657148063183, 0.11020754277706146, 0.8365734219551086, 0.03461247682571411, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4734065532684326, 0.14704622328281403, 0.05715098977088928, 0.25222980976104736, 0.07016641646623611, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49333417415618896, 0.008234936743974686, 0.008012527599930763, 0.021872591227293015, 0.03967061638832092, 0.428875207901001, 0.0, 0.0, 0.0, 0.0], [0.08060485869646072, 0.0033893154468387365, 0.002171718282625079, 0.0026488034054636955, 0.036696918308734894, 0.7740417718887329, 0.10044671595096588, 0.0, 0.0, 0.0], [0.08436581492424011, 0.008884111419320107, 0.006725000683218241, 0.013129651546478271, 0.01784602180123329, 0.1932034194469452, 0.1695568710565567, 0.5062890648841858, 0.0, 0.0], [0.13414521515369415, 0.014364570379257202, 0.00585120590403676, 0.01126515306532383, 0.00909710954874754, 0.16818936169147491, 0.3251395523548126, 0.2252425104379654, 0.10670536756515503, 0.0], [2.452119588269852e-05, 1.9645636712084524e-05, 0.00011441295646363869, 3.1429917726200074e-05, 5.5669035646133125e-05, 0.0013380550080910325, 0.02684668079018593, 0.0024312452878803015, 0.9657877087593079, 0.0033504932653158903]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5130534768104553, 0.48694655299186707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3975912034511566, 0.5228423476219177, 0.07956639677286148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23553597927093506, 0.22942191362380981, 0.20850297808647156, 0.32653912901878357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2904796898365021, 0.1287231743335724, 0.2951997220516205, 0.12991349399089813, 0.15568388998508453, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7625570297241211, 0.027590621262788773, 0.027350928634405136, 0.02580930106341839, 0.03493652120232582, 0.12175561487674713, 0.0, 0.0, 0.0, 0.0], [0.4462546408176422, 0.008537102490663528, 0.028795866295695305, 0.032935936003923416, 0.04105943813920021, 0.3229061961174011, 0.11951085925102234, 0.0, 0.0, 0.0], [0.28493931889533997, 0.011074136942625046, 0.009190265089273453, 0.019453972578048706, 0.026987304911017418, 0.19498902559280396, 0.349621057510376, 0.10374491661787033, 0.0, 0.0], [0.05542522296309471, 0.0641314908862114, 0.03741443157196045, 0.0557597354054451, 0.5587040185928345, 0.0592208206653595, 0.06786541640758514, 0.040136002004146576, 0.06134287267923355, 0.0], [0.18957604467868805, 0.007861909456551075, 0.010476202704012394, 0.012908213771879673, 0.010497424751520157, 0.1034943088889122, 0.07343605905771255, 0.4299052953720093, 0.02457478456199169, 0.13726970553398132]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8735913634300232, 0.1264086216688156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4011150896549225, 0.5105474591255188, 0.08833743631839752, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.027618370950222015, 0.05665247142314911, 0.8718303442001343, 0.04389885812997818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07733327895402908, 0.3653131127357483, 0.11332374811172485, 0.32430601119995117, 0.1197238638997078, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6556437611579895, 0.06300179660320282, 0.05083257704973221, 0.0788804367184639, 0.05136638134717941, 0.10027503967285156, 0.0, 0.0, 0.0, 0.0], [0.18654292821884155, 0.10325082391500473, 0.06530754268169403, 0.11005351692438126, 0.07516941428184509, 0.1905387043952942, 0.26913711428642273, 0.0, 0.0, 0.0], [0.09667760133743286, 0.04255342483520508, 0.023608200252056122, 0.056543491780757904, 0.048377860337495804, 0.12254257500171661, 0.33910518884658813, 0.270591676235199, 0.0, 0.0], [0.06725477427244186, 0.07902779430150986, 0.037737708538770676, 0.05301019921898842, 0.029360635206103325, 0.07956801354885101, 0.4394588768482208, 0.13144783675670624, 0.08313417434692383, 0.0], [0.06725261360406876, 0.03356018289923668, 0.02768505923449993, 0.05884742736816406, 0.03936830163002014, 0.08749471604824066, 0.1688593029975891, 0.1470056027173996, 0.1658822000026703, 0.20404452085494995]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8174959421157837, 0.1825040578842163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11346088349819183, 0.8076116442680359, 0.07892750203609467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.017064860090613365, 0.20547014474868774, 0.5517476797103882, 0.22571733593940735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011147839948534966, 0.02403853088617325, 0.7872782945632935, 0.0936041995882988, 0.0839310735464096, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007123135961592197, 0.03321104496717453, 0.11185067147016525, 0.11278776079416275, 0.44324788451194763, 0.291779488325119, 0.0, 0.0, 0.0, 0.0], [0.006628019735217094, 0.015841038897633553, 0.023362331092357635, 0.028282128274440765, 0.07155666500329971, 0.7880684733390808, 0.06626132130622864, 0.0, 0.0, 0.0], [0.0004611426265910268, 0.0012083295732736588, 0.001096363179385662, 0.0020610291976481676, 0.004222366493195295, 0.052768509835004807, 0.8986179232597351, 0.03956422582268715, 0.0, 0.0], [0.0044714403338730335, 0.0030466821044683456, 0.025206604972481728, 0.0184777844697237, 0.028084393590688705, 0.12872426211833954, 0.13313446938991547, 0.4756620228290558, 0.18319226801395416, 0.0], [0.00091141666052863, 0.0009552808478474617, 0.008818528614938259, 0.006785875651985407, 0.01171346940100193, 0.05196182429790497, 0.09358130395412445, 0.2109144628047943, 0.426074355840683, 0.18828347325325012]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3345666229724884, 0.665433406829834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2124827653169632, 0.46171835064888, 0.325798898935318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21691139042377472, 0.363395094871521, 0.2873225510120392, 0.1323709785938263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14811821281909943, 0.3364785313606262, 0.20350439846515656, 0.07528091967105865, 0.23661790788173676, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08503736555576324, 0.18161176145076752, 0.24255557358264923, 0.1580277532339096, 0.19204944372177124, 0.14071807265281677, 0.0, 0.0, 0.0, 0.0], [0.1245218813419342, 0.11159589886665344, 0.1399824321269989, 0.08270397782325745, 0.17941509187221527, 0.2354891002178192, 0.12629158794879913, 0.0, 0.0, 0.0], [0.028029654175043106, 0.06734573096036911, 0.09047392755746841, 0.048100363463163376, 0.1489553302526474, 0.1657056361436844, 0.4358976483345032, 0.01549173891544342, 0.0, 0.0], [0.08234745264053345, 0.2559697926044464, 0.08787793666124344, 0.08200085908174515, 0.24880775809288025, 0.08278200775384903, 0.0438232496380806, 0.0316946879029274, 0.0846962258219719, 0.0], [0.0520622581243515, 0.08785209059715271, 0.08251846581697464, 0.06066236272454262, 0.09179577976465225, 0.10067924857139587, 0.22894617915153503, 0.04295729473233223, 0.11332003772258759, 0.13920623064041138]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6521740555763245, 0.3478259742259979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47260844707489014, 0.4507277309894562, 0.07666382938623428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.45525529980659485, 0.1926717460155487, 0.05873100459575653, 0.2933419644832611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4720485210418701, 0.14024792611598969, 0.06089228391647339, 0.2702023386955261, 0.05660896375775337, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32822540402412415, 0.1246102899312973, 0.04554545879364014, 0.15255127847194672, 0.07841417193412781, 0.2706533968448639, 0.0, 0.0, 0.0, 0.0], [0.2873716652393341, 0.08126501739025116, 0.020196959376335144, 0.060919586569070816, 0.056354962289333344, 0.21469873189926147, 0.27919304370880127, 0.0, 0.0, 0.0], [0.20940187573432922, 0.09117225557565689, 0.03148096054792404, 0.06984073668718338, 0.044023770838975906, 0.08230166137218475, 0.1934521496295929, 0.2783266305923462, 0.0, 0.0], [0.19073554873466492, 0.024706248193979263, 0.004702771548181772, 0.009100218303501606, 0.011310776695609093, 0.261827677488327, 0.20821835100650787, 0.21922342479228973, 0.0701749324798584, 0.0], [0.1045900210738182, 0.01886243000626564, 0.0067127360962331295, 0.010519926436245441, 0.014393172226846218, 0.12164101004600525, 0.14099566638469696, 0.279011607170105, 0.04852485656738281, 0.2547486126422882]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5327021479606628, 0.4672977924346924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2894168496131897, 0.428524374961853, 0.2820587754249573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0027006505988538265, 0.00782750267535448, 0.9693534970283508, 0.020118290558457375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24669340252876282, 0.17147505283355713, 0.23301292955875397, 0.1221165582537651, 0.22670206427574158, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2555573284626007, 0.03965854272246361, 0.028971193358302116, 0.020663244649767876, 0.011494029313325882, 0.6436557173728943, 0.0, 0.0, 0.0, 0.0], [0.20796450972557068, 0.03158773481845856, 0.023883530870079994, 0.030746977776288986, 0.015544509515166283, 0.20087400078773499, 0.48939865827560425, 0.0, 0.0, 0.0], [0.07701205462217331, 0.010727901943027973, 0.005023232661187649, 0.008057613857090473, 0.0012516246642917395, 0.029886292293667793, 0.12402784079313278, 0.7440134286880493, 0.0, 0.0], [0.09852539747953415, 0.08819779008626938, 0.07811672240495682, 0.033274635672569275, 0.26161882281303406, 0.052245840430259705, 0.1673254519701004, 0.09561891853809357, 0.12507639825344086, 0.0], [0.0018261066870763898, 0.0012838448164984584, 0.0008565881289541721, 0.0016095238970592618, 0.004616647958755493, 0.002421535551548004, 0.003519008168950677, 0.005768938921391964, 0.9421509504318237, 0.035946886986494064]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4359164834022522, 0.5640835165977478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34310558438301086, 0.193734273314476, 0.46316006779670715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18442295491695404, 0.08524700254201889, 0.2432430535554886, 0.4870869815349579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21567164361476898, 0.13079501688480377, 0.24097099900245667, 0.2548925578594208, 0.1576698124408722, 0.0, 0.0, 0.0, 0.0, 0.0], [0.050237834453582764, 0.03873935341835022, 0.050004757940769196, 0.04733559116721153, 0.03742910549044609, 0.776253342628479, 0.0, 0.0, 0.0, 0.0], [0.05114845559000969, 0.04080699756741524, 0.06932128220796585, 0.03621926158666611, 0.03314264863729477, 0.1610209047794342, 0.6083404421806335, 0.0, 0.0, 0.0], [0.025111332535743713, 0.03258886560797691, 0.044793300330638885, 0.03725753352046013, 0.021802028641104698, 0.04561690241098404, 0.07765314728021622, 0.7151768803596497, 0.0, 0.0], [0.07588838785886765, 0.055973608046770096, 0.10367074608802795, 0.04044070094823837, 0.04906556010246277, 0.16095605492591858, 0.14304016530513763, 0.3384029269218445, 0.03256189450621605, 0.0], [0.040442340075969696, 0.06247076019644737, 0.12395961582660675, 0.08377137780189514, 0.06365847587585449, 0.07885753363370895, 0.07767991721630096, 0.16915485262870789, 0.04937927424907684, 0.2506258487701416]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8666063547134399, 0.13339364528656006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7037226557731628, 0.08050073683261871, 0.21577665209770203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5062465667724609, 0.06629222631454468, 0.06981618702411652, 0.35764503479003906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.37740546464920044, 0.028806498274207115, 0.1494835913181305, 0.16624043881893158, 0.27806395292282104, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21135656535625458, 0.042256638407707214, 0.13441215455532074, 0.10484889149665833, 0.3652103841304779, 0.14191538095474243, 0.0, 0.0, 0.0, 0.0], [0.10934919118881226, 0.02916078269481659, 0.10378804802894592, 0.07588699460029602, 0.5279759168624878, 0.05085797235369682, 0.10298112034797668, 0.0, 0.0, 0.0], [0.07791783660650253, 0.032187219709157944, 0.03131778538227081, 0.05185353383421898, 0.08924010396003723, 0.04645169898867607, 0.1844736784696579, 0.48655813932418823, 0.0, 0.0], [0.14273370802402496, 0.008835560642182827, 0.035214927047491074, 0.022769751027226448, 0.18999800086021423, 0.02845376916229725, 0.03436779975891113, 0.23312732577323914, 0.3044991195201874, 0.0], [0.06128959730267525, 0.017167722806334496, 0.07880358397960663, 0.03203364461660385, 0.2612563371658325, 0.018978141248226166, 0.03086722083389759, 0.11583200842142105, 0.23447981476783752, 0.14929191768169403]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8212168216705322, 0.17878317832946777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3396758735179901, 0.5219749808311462, 0.13834913074970245, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04702940583229065, 0.19365988671779633, 0.6941652297973633, 0.06514539569616318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0762493908405304, 0.04433031007647514, 0.04686227813363075, 0.8197378516197205, 0.012820110656321049, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2521655559539795, 0.07540450245141983, 0.0806838646531105, 0.11423364281654358, 0.168213352560997, 0.3092990219593048, 0.0, 0.0, 0.0, 0.0], [0.09420900791883469, 0.01816960610449314, 0.028360724449157715, 0.03727706894278526, 0.08318649977445602, 0.5231131315231323, 0.21568387746810913, 0.0, 0.0, 0.0], [0.07451336085796356, 0.02072478085756302, 0.023791689425706863, 0.0411953404545784, 0.053938958793878555, 0.26805758476257324, 0.19976764917373657, 0.318010538816452, 0.0, 0.0], [0.03059406764805317, 0.012348110787570477, 0.00996171124279499, 0.006671534851193428, 0.0034022685140371323, 0.06109631434082985, 0.1632358729839325, 0.6863061785697937, 0.026384005323052406, 0.0], [0.025209790095686913, 0.005476289428770542, 0.005883364472538233, 0.007687622681260109, 0.014250179752707481, 0.12825602293014526, 0.1338292509317398, 0.3083134591579437, 0.15664087235927582, 0.21445314586162567]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9453611373901367, 0.05463884398341179, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7012256979942322, 0.16970087587833405, 0.12907345592975616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7403304576873779, 0.11875211447477341, 0.036210622638463974, 0.10470681637525558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12056373804807663, 0.08851055055856705, 0.15055081248283386, 0.5414496064186096, 0.09892535209655762, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28596463799476624, 0.0679699182510376, 0.025052212178707123, 0.041540343314409256, 0.17584450542926788, 0.4036283493041992, 0.0, 0.0, 0.0, 0.0], [0.2957458198070526, 0.04547145962715149, 0.02355162426829338, 0.028333498165011406, 0.07833416759967804, 0.24397577345371246, 0.2845875918865204, 0.0, 0.0, 0.0], [0.08486074954271317, 0.025896485894918442, 0.01629001647233963, 0.008989562280476093, 0.014236284419894218, 0.18315383791923523, 0.23715601861476898, 0.42941710352897644, 0.0, 0.0], [0.04092155024409294, 0.048806264996528625, 0.01738102175295353, 0.010911516845226288, 0.2677488923072815, 0.06343573331832886, 0.30654680728912354, 0.126474991440773, 0.11777325719594955, 0.0], [0.14077208936214447, 0.02675667591392994, 0.005762417800724506, 0.009632841683924198, 0.013735946267843246, 0.07092709839344025, 0.1094210147857666, 0.35359957814216614, 0.0213389303535223, 0.24805346131324768]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9203739762306213, 0.07962600141763687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20258858799934387, 0.6306097507476807, 0.16680166125297546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002432319801300764, 0.0018929652869701385, 0.9721331596374512, 0.02354150265455246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2419133484363556, 0.005510878283530474, 0.17049086093902588, 0.2913920283317566, 0.29069286584854126, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24400991201400757, 0.009351830929517746, 0.0177416130900383, 0.03455575555562973, 0.009871178306639194, 0.6844696402549744, 0.0, 0.0, 0.0, 0.0], [0.14732977747917175, 0.015364331193268299, 0.004563696216791868, 0.011315342970192432, 0.043744493275880814, 0.5253489017486572, 0.2523334324359894, 0.0, 0.0, 0.0], [0.07099173963069916, 0.002983162645250559, 0.008780461736023426, 0.010933419689536095, 0.012722334824502468, 0.13144055008888245, 0.16819895803928375, 0.5939492583274841, 0.0, 0.0], [0.034700773656368256, 0.0020962192211300135, 0.003587249666452408, 0.004411224741488695, 0.3381350636482239, 0.19166678190231323, 0.040754564106464386, 0.20953278243541718, 0.17511534690856934, 0.0], [0.04594598338007927, 0.0035846976097673178, 0.0035615027882158756, 0.01369266677647829, 0.022732658311724663, 0.04340611398220062, 0.04250757768750191, 0.2669268548488617, 0.14452892541885376, 0.413112998008728]]]}\n",
              "    )\n",
              "    </script>"
            ],
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f68356ad010>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get string tokens (replacing special character for spaces)\n",
        "str_tokens = model.tokenizer.tokenize(prompt)\n",
        "str_tokens = [s.replace('Ġ', ' ') for s in str_tokens]\n",
        "\n",
        "# Attention patterns (squeeze out the batch dimension)\n",
        "attn_patterns_value = attn_patterns.value[0].squeeze(0)\n",
        "\n",
        "print(\"Layer 0 Head Attention Patterns:\")\n",
        "display(cv.attention.attention_patterns(\n",
        "    tokens=str_tokens,\n",
        "    attention=attn_patterns_value,\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "435a9f8f",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Solution (and explanation)</summary>\n",
        "\n",
        "```python\n",
        "with model.generate(max_new_tokens=1, pad_token_id=tokenizer.eos_token_id) as generator:\n",
        "    with generator.invoke(prompt) as invoker:\n",
        "        attn_patterns = model.transformer.h[0].attn.attn_dropout.input.save()\n",
        "\n",
        "str_tokens = model.tokenizer.tokenize(prompt)\n",
        "\n",
        "# Attention patterns (squeeze out the batch dimension)\n",
        "attn_patterns = attn_patterns.value[0].squeeze(0)\n",
        "\n",
        "print(\"Layer 0 Head Attention Patterns:\")\n",
        "display(cv.attention.attention_patterns(\n",
        "    tokens=str_tokens,\n",
        "    attention=attn_patterns,\n",
        "))\n",
        "```\n",
        "\n",
        "Explanation:\n",
        "\n",
        "* Within the context managers:\n",
        "    * We access the attention patterns by taking the input to the `attn_dropout`.\n",
        "        * From the GPT-J source code, we can see that the attention weights are calculated by standard torch functions (and an unnamed `nn.Softmax` module) from the key and query vectors, and are then passed through the dropout layer before being used to calculate the attention layer output. So by accessing the input to the dropdown layer, we get the attention weights before dropout is applied.\n",
        "        * Because of the previously discussed point about dropout not working in inference mode, we could also use the output of `attn_dropout`, and get the same values.\n",
        "    * We use the `.save()` method to save the attention patterns (as an object).\n",
        "* Outside of the context managers:\n",
        "    * We use the `tokenize` method to tokenize the prompt.\n",
        "    * We use the `.value` to access the actual value of the intervention proxy `attn_patterns`.\n",
        "        * This returns a tuple of length-1, so we index into it to get the actual tensor, then squeeze to remove the batch dimension.\n",
        "        \n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4234f7a-2f53-482f-885d-5b32ed682989",
      "metadata": {
        "id": "f4234f7a-2f53-482f-885d-5b32ed682989"
      },
      "source": [
        "As an optional bonus exercise, you can verify for yourself that these are the correct attention patterns, by calculating them from scratch using the key and query vectors. Using `model.transformer.h[0].attn.q_proj.output` will give you the query vectors, and `k_proj` for the key vectors. However, one thing to be wary of is that GPT-J uses **rotary embeddings**, which makes the computation of attention patterns from keys and queries a bit harder than it would otherwise be. See [here](https://blog.eleuther.ai/rotary-embeddings/) for an in-depth discussion of rotary embeddings, and [here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#q=rotary) for some rough intuitions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886c7964",
      "metadata": {},
      "source": [
        "# 2️⃣ Task-encoding hidden states"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9867d5ae",
      "metadata": {},
      "source": [
        "(Note - this section structurally follows section 2.1 of the function vectors paper).\n",
        "\n",
        "We begin with the following question:\n",
        "\n",
        "> When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task $T$, do any hidden states encode the task itself?\n",
        "\n",
        "Throughout these exercises, we'll be focusing on the **antonyms task**. In other words, given a prompt which includes a bunch of antonym pairs, ending with a single word, what causes the model to complete this prompt with an antonym? Is there a residual stream state that encodes the \"antonym task\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2724827c",
      "metadata": {},
      "source": [
        "### Exercise (optional) - generate your own antonym pairs\n",
        "\n",
        "```c\n",
        "Difficulty: 🔴🔴🔴🔴⚪\n",
        "Importance: 🔵⚪⚪⚪⚪\n",
        "\n",
        "You should spend up to 10-30 minutes on this exercise - depending on your familiarity with the OpenAI Python API.\n",
        "```\n",
        "\n",
        "We've provided you a list of word pairs, in the file `data/antonym_pairs.txt`. **Optionally, you can just skip this exercise, and run the code below to load these words in.**\n",
        "\n",
        "Alternatively, if you want to run experiments like the ones in this paper, it can be good practice to learn how to generate prompts from GPT-4 or other models (this is how we generated the data for this exercise). To do this, you can fill in the `generate_dataset` function below, which should query GPT-4 and get a list of antonym pairs. See [here](https://platform.openai.com/docs/guides/gpt/chat-completions-api) for a guide to using the chat completions API, if you haven't already used it.\n",
        "\n",
        "Use the two dropdowns below (in order) for some guidance.\n",
        "\n",
        "<details>\n",
        "<summary>Getting started</summary>\n",
        "\n",
        "Here is a recommended template:\n",
        "\n",
        "```python\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": antonym_task},\n",
        "        {\"role\": \"assistant\", \"content\": start_of_response},\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "where `antonym_task` explains the antonym task, and `start_of_respose` gives the model a prompt to start from (e.g. \"Sure, here are some antonyms: ...\"), to guide its subsequent behaviour.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Getting started</summary>\n",
        "\n",
        "Here is an template you might want to use for the actual request:\n",
        "\n",
        "```python\n",
        "example_antonyms = \"old:young, top:bottom, awake:asleep, future:past, \"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Give me {N} examples of antonym pairs. They should be obvious, i.e. each word should be associated with a single correct antonym.\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Sure! Here are {N} pairs of antonyms: {example_antonyms}\"},\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "where `N` is the function argument. Note that we've provided a few example antonyms, and appended them to the start of GPT4's completion. This is a classic trick to guide the rest of the output (in fact, it's commonly used in adversarial attacks).\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b39766f8",
      "metadata": {},
      "source": [
        "Note - it's possible that not all the antonyms returned will be solvable by GPT-J. In this section, we won't worry too much about this. When it comes to testing out our zero-shot intervention, we'll make sure to only use cases where GPT-J can actually solve it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "83fcb573",
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_key = \"<your-key-here>\"\n",
        "\n",
        "def generate_dataset(N: int):\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Define a few examples (for our dataset, and for our prompt)\n",
        "    example_antonyms = \"old:young, top:bottom, awake:asleep, future:past, \"\n",
        "\n",
        "    # Use openai's api to generate examples. We prepend the example antonyms to the assistant's response, to both\n",
        "    # make sure the query is successful, and so that the assistant returns words in the same syntax as the examples.\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Give me {N} examples of antonym pairs. They should be obvious, i.e. each word should be associated with a single correct antonym.\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"Sure! Here are {N} pairs of antonyms satiisfying this specification: {example_antonyms}\"},\n",
        "        ]\n",
        "    )\n",
        "    # Add our examples to the response\n",
        "    response_text: str = example_antonyms + response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    # Create word pairs, by splitting on commas and colons\n",
        "    word_pairs = [word_pair.split(\":\") for word_pair in response_text.strip(\".\\n\").split(\", \")]\n",
        "\n",
        "    print(f\"Finished in {time.time()-t0:.2f} seconds.\")\n",
        "\n",
        "    return word_pairs\n",
        "\n",
        "\n",
        "# WORD_PAIRS = generate_dataset(100)\n",
        "\n",
        "# # save the word pairs in a text file\n",
        "# with open(root / \"data\" / \"antonym_pairs.txt\", \"w\") as f:\n",
        "#     for word_pair in WORD_PAIRS:\n",
        "#         f.write(f\"{word_pair[0]} {word_pair[1]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4494f16f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the word pairs from the text file\n",
        "with open(root / \"data\" / \"antonym_pairs.txt\", \"r\") as f:\n",
        "    WORD_PAIRS = [line.split() for line in f.readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ab53287",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['old', 'young'],\n",
              " ['top', 'bottom'],\n",
              " ['awake', 'asleep'],\n",
              " ['future', 'past'],\n",
              " ['beginning', 'end'],\n",
              " ['volunteer', 'compel'],\n",
              " ['best', 'worst'],\n",
              " ['big', 'small'],\n",
              " ['boring', 'exciting'],\n",
              " ['brave', 'cowardly']]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "WORD_PAIRS[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fecc199e",
      "metadata": {},
      "source": [
        "## Antonym Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee713227",
      "metadata": {},
      "source": [
        "To handle this list of word pairs, we've given you some helpful classes.\n",
        "\n",
        "Firstly, there's the `AntonymSequence` class, which takes in a list of word pairs and contains methods for constructing a prompt (and completion) from these words. Run the code below to see how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f477b99a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuple-representation of the sequence:\n",
            "(hot, cold), (yes, no), (in, out), up ->\n",
            "\n",
            "Actual prompt, which will be fed into the model:\n",
            "Q: hot\n",
            "A: cold\n",
            "\n",
            "Q: yes\n",
            "A: no\n",
            "\n",
            "Q: in\n",
            "A: out\n",
            "\n",
            "Q: up\n",
            "A:\n"
          ]
        }
      ],
      "source": [
        "class AntonymSequence:\n",
        "    '''\n",
        "    Class to store a single antonym sequence.\n",
        "\n",
        "    Uses the default template \"Q: {x}\\nA: {y}\" (with separate pairs split by \"\\n\\n\").\n",
        "    '''\n",
        "    def __init__(self, word_pairs: List[List[str]]):\n",
        "        self.word_pairs = word_pairs\n",
        "        self.x, self.y = zip(*word_pairs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word_pairs)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return self.word_pairs[idx]\n",
        "\n",
        "    def prompt(self):\n",
        "        '''Returns the prompt, which contains all but the second element in the last word pair.'''\n",
        "        p = \"\\n\\n\".join([f\"Q: {x}\\nA: {y}\" for x, y in self.word_pairs])\n",
        "        return p[:-len(self.completion())]\n",
        "\n",
        "    def completion(self):\n",
        "        '''Returns the second element in the last word pair (with padded space).'''\n",
        "        return \" \" + self.y[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        '''Prints a readable string representation of the prompt & completion (indep of template).'''\n",
        "        return f\"{', '.join([f'({x}, {y})' for x, y in self[:-1]])}, {self.x[-1]} ->\".strip(\", \")\n",
        "\n",
        "\n",
        "word_list = [[\"hot\", \"cold\"], [\"yes\", \"no\"], [\"in\", \"out\"], [\"up\", \"down\"]]\n",
        "seq = AntonymSequence(word_list)\n",
        "\n",
        "print(\"Tuple-representation of the sequence:\")\n",
        "print(seq)\n",
        "print(\"\\nActual prompt, which will be fed into the model:\")\n",
        "print(seq.prompt())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "702cb3db",
      "metadata": {},
      "source": [
        "Secondly, we have the `AntonymDataset` class. This is also fed a word pair list, and it has methods for generating batches of prompts and completions. It can generate both clean prompts (where each pair is actually an antonym pair) and corrupted prompts (where the answers for each pair are randomly chosen from the dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "b0b4eea8",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AntonymDataset:\n",
        "    '''\n",
        "    Dataset to create antonym pair prompts, in ICL task format. We use random seeds for consistency\n",
        "    between the corrupted and clean datasets.\n",
        "\n",
        "    Inputs:\n",
        "        word_pairs:\n",
        "            list of ICL task, e.g. [[\"old\", \"young\"], [\"top\", \"bottom\"], ...] for the antonym task\n",
        "        size:\n",
        "            number of prompts to generate\n",
        "        n_prepended:\n",
        "            number of antonym pairs before the single-word ICL task\n",
        "        bidirectional:\n",
        "            if True, then we also consider the reversed antonym pairs\n",
        "        corrupted:\n",
        "            if True, then the second word in each pair is replaced with a random word\n",
        "        seed:\n",
        "            random seed, for consistency & reproducibility\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        word_pairs: List[List[str]],\n",
        "        size: int,\n",
        "        n_prepended: int,\n",
        "        bidirectional: bool = True,\n",
        "        corrupted: bool = False,\n",
        "        seed: int = 0,\n",
        "    ):\n",
        "        assert n_prepended+1 <= len(word_pairs), \"Not enough antonym pairs in dataset to create prompt.\"\n",
        "        \n",
        "        self.word_pairs = word_pairs\n",
        "        self.word_list = [word for word_pair in word_pairs for word in word_pair]\n",
        "        self.size = size\n",
        "        self.n_prepended = n_prepended\n",
        "        self.bidirectional = bidirectional\n",
        "        self.corrupted = corrupted\n",
        "        self.seed = seed\n",
        "\n",
        "        self.seqs = []\n",
        "        self.prompts = []\n",
        "        self.completions = []\n",
        "\n",
        "        # Generate the dataset (by choosing random antonym pairs, and constructing `AntonymSequence` objects)\n",
        "        for n in range(size):\n",
        "            np.random.seed(seed + n)\n",
        "            random_pairs = np.random.choice(len(self.word_pairs), n_prepended+1, replace=False)\n",
        "            random_orders = np.random.choice([1, -1], n_prepended+1)\n",
        "            if not(bidirectional): random_orders[:] = 1\n",
        "            word_pairs = [self.word_pairs[pair][::order] for pair, order in zip(random_pairs, random_orders)]\n",
        "            if corrupted:\n",
        "                for i in range(len(word_pairs) - 1):\n",
        "                    word_pairs[i][1] = np.random.choice(self.word_list)\n",
        "            seq = AntonymSequence(word_pairs)\n",
        "\n",
        "            self.seqs.append(seq)\n",
        "            self.prompts.append(seq.prompt())\n",
        "            self.completions.append(seq.completion())\n",
        "\n",
        "    def create_corrupted_dataset(self):\n",
        "        '''Creates a corrupted version of the dataset (with same random seed).'''\n",
        "        return AntonymDataset(self.word_pairs, self.size, self.n_prepended, True, self.seed)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return self.seqs[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdcd4b89",
      "metadata": {},
      "source": [
        "You can see how this dataset works below. **Note that the correct completions have a prepended space**, because this is how the antonym prompts are structured - the answers are tokenized as `\"A: answer\" -> [\"A\", \":\", \" answer\"]`. Forgetting prepended spaces is a classic mistake when working with transformers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cda6ba22",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Prompt                                                   </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ (comedy, tragedy), (harmony, discord), asleep -&gt;         │ ' awake'           │\n",
              "│ (vertical, horizontal), (courage, cowardice), inflate -&gt; │ ' deflate'         │\n",
              "│ (marry, divorce), (tenant, landlord), discord -&gt;         │ ' harmony'         │\n",
              "│ (angel, devil), (compete, cooperate), giant -&gt;           │ ' dwarf'           │\n",
              "│ (liberate, confine), (horizontal, vertical), human -&gt;    │ ' animal'          │\n",
              "│ (noisy, silent), (before, after), benefit -&gt;             │ ' disadvantage'    │\n",
              "│ (separate, join), (domestic, foreign), old -&gt;            │ ' young'           │\n",
              "│ (unfaithful, faithful), (minimum, maximum), solid -&gt;     │ ' liquid'          │\n",
              "│ (courage, cowardice), (bottom, top), honest -&gt;           │ ' deceitful'       │\n",
              "│ (light, heavy), (deflate, inflate), empty -&gt;             │ ' full'            │\n",
              "└──────────────────────────────────────────────────────────┴────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mPrompt                                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ (comedy, tragedy), (harmony, discord), asleep ->         │ ' awake'           │\n",
              "│ (vertical, horizontal), (courage, cowardice), inflate -> │ ' deflate'         │\n",
              "│ (marry, divorce), (tenant, landlord), discord ->         │ ' harmony'         │\n",
              "│ (angel, devil), (compete, cooperate), giant ->           │ ' dwarf'           │\n",
              "│ (liberate, confine), (horizontal, vertical), human ->    │ ' animal'          │\n",
              "│ (noisy, silent), (before, after), benefit ->             │ ' disadvantage'    │\n",
              "│ (separate, join), (domestic, foreign), old ->            │ ' young'           │\n",
              "│ (unfaithful, faithful), (minimum, maximum), solid ->     │ ' liquid'          │\n",
              "│ (courage, cowardice), (bottom, top), honest ->           │ ' deceitful'       │\n",
              "│ (light, heavy), (deflate, inflate), empty ->             │ ' full'            │\n",
              "└──────────────────────────────────────────────────────────┴────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = AntonymDataset(WORD_PAIRS, size=10, n_prepended=2, corrupted=False)\n",
        "\n",
        "table = Table(\"Prompt\", \"Correct completion\")\n",
        "for seq, completion in zip(dataset.seqs, dataset.completions):\n",
        "    table.add_row(str(seq), repr(completion))\n",
        "\n",
        "rprint(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8083654d",
      "metadata": {},
      "source": [
        "Compare this output to what it looks like when `corrupted=True`. You'll see the second elements of each pair change to a random word, but the first elements (and the final pair) stay the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "12ba4c23",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Prompt                                               </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ (comedy, vertical), (harmony, faithful), asleep -&gt;   │ ' awake'           │\n",
              "│ (vertical, after), (courage, angel), inflate -&gt;      │ ' deflate'         │\n",
              "│ (marry, unconscious), (tenant, landlord), discord -&gt; │ ' harmony'         │\n",
              "│ (angel, hard), (compete, departure), giant -&gt;        │ ' dwarf'           │\n",
              "│ (liberate, below), (horizontal, exterior), human -&gt;  │ ' animal'          │\n",
              "│ (noisy, knowledge), (before, comedy), benefit -&gt;     │ ' disadvantage'    │\n",
              "│ (separate, empty), (domestic, wisdom), old -&gt;        │ ' young'           │\n",
              "│ (unfaithful, interior), (minimum, win), solid -&gt;     │ ' liquid'          │\n",
              "│ (courage, criminal), (bottom, slave), honest -&gt;      │ ' deceitful'       │\n",
              "│ (light, folly), (deflate, guilty), empty -&gt;          │ ' full'            │\n",
              "└──────────────────────────────────────────────────────┴────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mPrompt                                              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ (comedy, vertical), (harmony, faithful), asleep ->   │ ' awake'           │\n",
              "│ (vertical, after), (courage, angel), inflate ->      │ ' deflate'         │\n",
              "│ (marry, unconscious), (tenant, landlord), discord -> │ ' harmony'         │\n",
              "│ (angel, hard), (compete, departure), giant ->        │ ' dwarf'           │\n",
              "│ (liberate, below), (horizontal, exterior), human ->  │ ' animal'          │\n",
              "│ (noisy, knowledge), (before, comedy), benefit ->     │ ' disadvantage'    │\n",
              "│ (separate, empty), (domestic, wisdom), old ->        │ ' young'           │\n",
              "│ (unfaithful, interior), (minimum, win), solid ->     │ ' liquid'          │\n",
              "│ (courage, criminal), (bottom, slave), honest ->      │ ' deceitful'       │\n",
              "│ (light, folly), (deflate, guilty), empty ->          │ ' full'            │\n",
              "└──────────────────────────────────────────────────────┴────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = AntonymDataset(WORD_PAIRS, size=10, n_prepended=2, corrupted=True)\n",
        "\n",
        "table = Table(\"Prompt\", \"Correct completion\")\n",
        "for seq, completions in zip(dataset.seqs, dataset.completions):\n",
        "    table.add_row(str(seq), repr(completions))\n",
        "\n",
        "rprint(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a8d9e3",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Aside - the <code>rich</code> library</summary>\n",
        "\n",
        "The `rich` library is a helpful little library to display outputs more clearly in a Python notebook or terminal. It's not necessary for this workshop, but it's a nice little tool to have in your toolbox.\n",
        "\n",
        "The most important function is `rich.print` (usually imported as `rprint`). This can print basic strings, but it also supports the following syntax for printing colors:\n",
        "\n",
        "```python\n",
        "rprint(\"[green]This is green text[/], this is default color\")\n",
        "```\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-1.png\" width=\"350\">\n",
        "\n",
        "and for making text bold / underlined:\n",
        "\n",
        "```python\n",
        "rprint(\"[u dark_orange]This is underlined[/], and [b cyan]this is bold[/].\")\n",
        "```\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-2.png\" width=\"350\">\n",
        "\n",
        "It can also print tables:\n",
        "\n",
        "```python\n",
        "from rich.table import Table\n",
        "\n",
        "table = Table(\"Col1\", \"Col2\", title=\"Title\") # title is optional\n",
        "table.add_row(\"A\", \"a\")\n",
        "table.add_row(\"B\", \"b\")\n",
        "\n",
        "rprint(table)\n",
        "```\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-3.png\" width=\"150\">\n",
        "\n",
        "The text formatting (bold, underlined, colors, etc) is also supported within table cells.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c466e56a",
      "metadata": {},
      "source": [
        "### Exercise - run forward pass on antonym dataset\n",
        "\n",
        "```c\n",
        "Difficulty: 🔴🔴⚪⚪⚪\n",
        "Importance: 🔵🔵🔵⚪⚪\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n",
        "\n",
        "You should fill in the `calculate_h` function below. It should:\n",
        "\n",
        "* Generate `N` random prompts from the antonym dataset (using the `create_prompts` method),\n",
        "* Run a forward pass on the model, using the `nnsight` syntax we've demonstrated previously,\n",
        "* Return a tuple of the model's output (i.e. its tokens) and the residual stream value at the end of layer `layer` (e.g. if `layer = -1`, this means the final value of the residual stream before we convert into logits).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-1.png\" width=\"900\">\n",
        "\n",
        "You should only return the residual stream values for the very last sequence position in each prompt, i.e. the last `:` token (where the model makes the antonym prediction).\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm not sure how to run (and index into) a batch of inputs.</summary>\n",
        "\n",
        "If we pass a list of strings to the `generator.invoke` function, this will be tokenized with padding automatically.\n",
        "\n",
        "The type of padding which is applied is **left padding**, meaning if you index at sequence position `-1`, this will get the final token in the prompt for all prompts in the list, even if the prompts have different lengths.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "21a77af1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests in `test_calculate_h` passed.\n"
          ]
        }
      ],
      "source": [
        "def calculate_h(dataset: AntonymDataset, layer: int = -1) -> Tuple[List[str], Tensor]:\n",
        "    '''\n",
        "    Generates N random sequences of the form \"old:young, vanish:appear, dark:\", but with 3 randomly chosen pairs (and orders).\n",
        "\n",
        "    Averages over the hidden states of te last layer of GPT-J for each token in each sequence.\n",
        "\n",
        "    Returns:\n",
        "        completions: list of model completion strings (i.e. the strings the model predicts to follow the last token)\n",
        "        h: average hidden state tensor at final sequence position, of shape (d_model,)\n",
        "    '''\n",
        "    with model.generate(max_new_tokens=1, pad_token_id=tokenizer.eos_token_id) as generator:\n",
        "        with generator.invoke(dataset.prompts) as invoker:\n",
        "            hidden_states = model.transformer.h[layer].output[0][:, -1].save()\n",
        "\n",
        "    completions = generator.output[:, -1]\n",
        "    completions = model.tokenizer.batch_decode(completions)\n",
        "    h = hidden_states.value.mean(dim=0)\n",
        "\n",
        "    return completions, h\n",
        "\n",
        "\n",
        "tests.test_calculate_h(calculate_h)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e8b294d",
      "metadata": {},
      "source": [
        "We've provided you with a helper function, which displays the model's output on the antonym dataset (and highlights the examples where the model's prediction is correct).\n",
        "\n",
        "If you've constructed your antonyms dataset well, you should find that the model's completion is correct most of the time, and most of its mistakes are understandable (e.g. predicting `weak` rather than `fragile` as the antonym of `strong`). If we were being rigorous, we'd want to filter this dataset to make sure it only contains examples where the model can correctly perform the task - but here, we won't worry about this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c7e4b849",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_model_completions_on_antonyms(\n",
        "    dataset: AntonymDataset,\n",
        "    completions: List[str],\n",
        "    num_to_display: int = 20,\n",
        ") -> None:\n",
        "    table = Table(\"Prompt\", \"Model's completion\", \"Correct completion\", title=\"Model's antonym completions (green = first token is a match)\")\n",
        "\n",
        "    for i in range(min(len(completions), num_to_display)):\n",
        "\n",
        "        # Get model's completion, and correct completion\n",
        "        completion = completions[i]\n",
        "        correct_completion = dataset.completions[i]\n",
        "        correct_completion_first_token = model.tokenizer.tokenize(correct_completion)[0].replace('Ġ', ' ')\n",
        "        seq = dataset.seqs[i]\n",
        "        \n",
        "        # Color code the completion based on whether it's correct\n",
        "        is_correct = (completion == correct_completion_first_token)\n",
        "        completion = f\"[b green]{repr(completion)}[/]\" if is_correct else repr(completion)\n",
        "\n",
        "        table.add_row(str(seq), completion, repr(correct_completion))\n",
        "\n",
        "    rprint(table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "19f556fb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                  Model's antonym completions (green = first token is a match)                   </span>\n",
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Prompt                                              </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ (awake, asleep), (sad, happy), pass -&gt;              │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' fail'</span>            │ ' fail'            │\n",
              "│ (first, last), (healthy, sick), major -&gt;            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' minor'</span>           │ ' minor'           │\n",
              "│ (increase, decrease), (slavery, freedom), closed -&gt; │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' open'</span>            │ ' open'            │\n",
              "│ (fantasy, reality), (domestic, wild), brave -&gt;      │ ' afraid'          │ ' cowardly'        │\n",
              "│ (loose, tight), (careful, careless), calm -&gt;        │ ' calm'            │ ' stormy'          │\n",
              "│ (first, last), (exterior, interior), south -&gt;       │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' north'</span>           │ ' north'           │\n",
              "│ (paradise, hell), (courage, fear), deep -&gt;          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' shallow'</span>         │ ' shallow'         │\n",
              "│ (courage, fear), (include, exclude), up -&gt;          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' down'</span>            │ ' down'            │\n",
              "│ (future, past), (bottom, top), thin -&gt;              │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' thick'</span>           │ ' thick'           │\n",
              "│ (worst, best), (maximum, minimum), fast -&gt;          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' slow'</span>            │ ' slow'            │\n",
              "│ (hot, cold), (reject, accept), past -&gt;              │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' future'</span>          │ ' future'          │\n",
              "│ (reject, accept), (modern, ancient), dark -&gt;        │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' light'</span>           │ ' light'           │\n",
              "│ (sane, insane), (exciting, boring), careful -&gt;      │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' careless'</span>        │ ' careless'        │\n",
              "│ (rough, gentle), (buy, sell), insane -&gt;             │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' sane'</span>            │ ' sane'            │\n",
              "│ (moral, immoral), (fear, courage), humble -&gt;        │ ' proud'           │ ' arrogant'        │\n",
              "│ (arrive, depart), (young, old), join -&gt;             │ ' leave'           │ ' separate'        │\n",
              "│ (low, high), (rough, gentle), immoral -&gt;            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' moral'</span>           │ ' moral'           │\n",
              "│ (stupid, smart), (combine, separate), brave -&gt;      │ ' stupid'          │ ' cowardly'        │\n",
              "│ (courage, fear), (junior, senior), shallow -&gt;       │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' deep'</span>            │ ' deep'            │\n",
              "│ (inferior, superior), (exclude, include), yes -&gt;    │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' no'</span>              │ ' no'              │\n",
              "└─────────────────────────────────────────────────────┴────────────────────┴────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                  Model's antonym completions (green = first token is a match)                   \u001b[0m\n",
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mPrompt                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ (awake, asleep), (sad, happy), pass ->              │ \u001b[1;32m' fail'\u001b[0m            │ ' fail'            │\n",
              "│ (first, last), (healthy, sick), major ->            │ \u001b[1;32m' minor'\u001b[0m           │ ' minor'           │\n",
              "│ (increase, decrease), (slavery, freedom), closed -> │ \u001b[1;32m' open'\u001b[0m            │ ' open'            │\n",
              "│ (fantasy, reality), (domestic, wild), brave ->      │ ' afraid'          │ ' cowardly'        │\n",
              "│ (loose, tight), (careful, careless), calm ->        │ ' calm'            │ ' stormy'          │\n",
              "│ (first, last), (exterior, interior), south ->       │ \u001b[1;32m' north'\u001b[0m           │ ' north'           │\n",
              "│ (paradise, hell), (courage, fear), deep ->          │ \u001b[1;32m' shallow'\u001b[0m         │ ' shallow'         │\n",
              "│ (courage, fear), (include, exclude), up ->          │ \u001b[1;32m' down'\u001b[0m            │ ' down'            │\n",
              "│ (future, past), (bottom, top), thin ->              │ \u001b[1;32m' thick'\u001b[0m           │ ' thick'           │\n",
              "│ (worst, best), (maximum, minimum), fast ->          │ \u001b[1;32m' slow'\u001b[0m            │ ' slow'            │\n",
              "│ (hot, cold), (reject, accept), past ->              │ \u001b[1;32m' future'\u001b[0m          │ ' future'          │\n",
              "│ (reject, accept), (modern, ancient), dark ->        │ \u001b[1;32m' light'\u001b[0m           │ ' light'           │\n",
              "│ (sane, insane), (exciting, boring), careful ->      │ \u001b[1;32m' careless'\u001b[0m        │ ' careless'        │\n",
              "│ (rough, gentle), (buy, sell), insane ->             │ \u001b[1;32m' sane'\u001b[0m            │ ' sane'            │\n",
              "│ (moral, immoral), (fear, courage), humble ->        │ ' proud'           │ ' arrogant'        │\n",
              "│ (arrive, depart), (young, old), join ->             │ ' leave'           │ ' separate'        │\n",
              "│ (low, high), (rough, gentle), immoral ->            │ \u001b[1;32m' moral'\u001b[0m           │ ' moral'           │\n",
              "│ (stupid, smart), (combine, separate), brave ->      │ ' stupid'          │ ' cowardly'        │\n",
              "│ (courage, fear), (junior, senior), shallow ->       │ \u001b[1;32m' deep'\u001b[0m            │ ' deep'            │\n",
              "│ (inferior, superior), (exclude, include), yes ->    │ \u001b[1;32m' no'\u001b[0m              │ ' no'              │\n",
              "└─────────────────────────────────────────────────────┴────────────────────┴────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get uncorrupted dataset\n",
        "dataset = AntonymDataset(WORD_PAIRS, size=20, n_prepended=2)\n",
        "\n",
        "# Getting it from layer 12, cause the graph suggested this was where there was high accuracy\n",
        "model_completions, h = calculate_h(dataset, layer=12)\n",
        "\n",
        "# Displaying the output\n",
        "display_model_completions_on_antonyms(dataset, model_completions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3525cf03",
      "metadata": {},
      "source": [
        "### Exercise - intervene with $h$\n",
        "\n",
        "```c\n",
        "Difficulty: 🔴🔴🔴⚪⚪\n",
        "Importance: 🔵🔵🔵🔵⚪\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n",
        "\n",
        "You should fill in the function `intervene_with_h` below. This will involve:\n",
        "\n",
        "* Using the `calculate_h` function you just wrote to get the h-vector (this code is already filled in below),\n",
        "* Defining a zero-shot dataset, i.e. with no prepended antonym pairs,\n",
        "* Run two forward passes (within the same context manager):\n",
        "    * One with no intervention (i.e. `h` is unchanged),\n",
        "    * One with an intervention on `h` (i.e. the residual stream value is set to `h`, at the layer which `h` was taken from).\n",
        "* Return the zero-shot dataset, as well as the completions for no intervention and intervention cases respectively (see docstring).\n",
        "\n",
        "The diagram below shows how all of this should work, when combined with the `calculate_h` function.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-2.png\" width=\"950\">\n",
        "\n",
        "Hint - you can use `tokenizer.batch_decode` to turn a list of tokens into a list of strings.\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm not sure how best to get both the no-intervention and intervention completions.</summary>\n",
        "\n",
        "You can use `with generator.invoke...` more than once within the same context manager, in order to add to your batch. This will eventually give you output of shape (2*N, seq_len), which can then be indexed and reshaped to get the completions in the no intervention & intervention cases respectively.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm not sure how to intervene on the hidden state.</summary>\n",
        "\n",
        "First, you can define the tensor of hidden states (i.e. using `.output[0]`, like you've done before).\n",
        "\n",
        "Then, you can add to this tensor directly (or add to some indexed version of it). You can use inplace operations (i.e. `tensor += h`) or redefining the tensor (i.e. `tensor = tensor + h`); either work.\n",
        "\n",
        "You won't need to `.save()` anything here; we're just intervening rather than storing the value of the residual stream.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c31eccf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def intervene_with_h(\n",
        "    dataset: AntonymDataset,\n",
        "    layer: int,\n",
        "    zero_shot_size: int,\n",
        ") -> Tuple[AntonymDataset, List[str]]:\n",
        "    '''\n",
        "    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the\n",
        "    residual stream of a set of generated zero-shot prompts.\n",
        "\n",
        "    Inputs:\n",
        "        word_list: the list of words used to create the prompts\n",
        "        dataset: the dataset of clean prompts from which we'll extract the `h`-vector\n",
        "        layer: the layer we'll be extracting the `h`-vector from\n",
        "        zero_shot_size: the number of zero-shot prompts to generate, to test our intervention\n",
        "    \n",
        "    Returns:\n",
        "        zero_shot_dataset: the dataset of zero-shot prompts, which you should generate in this fn\n",
        "        completions: list of string completions for the zero-shot prompts, without intervention\n",
        "        completions_intervention: list of string completions for the zero-shot prompts, with h-intervention\n",
        "    '''\n",
        "    # Run previous function to get h-vector\n",
        "    h = calculate_h(dataset, layer=layer)[1]\n",
        "    \n",
        "    # Get zero-shot dataset\n",
        "    zero_shot_dataset = AntonymDataset(WORD_PAIRS, size=zero_shot_size, n_prepended=0)\n",
        "\n",
        "    with model.generate(max_new_tokens=1, pad_token_id=tokenizer.eos_token_id) as generator:\n",
        "\n",
        "        # First, run a forward pass where we don't intervene\n",
        "        with generator.invoke(zero_shot_dataset.prompts) as invoker:\n",
        "            pass\n",
        "\n",
        "        # Next, run a forward pass on the zero-shot prompts where we do intervene\n",
        "        with generator.invoke(zero_shot_dataset.prompts) as invoker:\n",
        "            # Access the tensor (which is the first element of the output tuple)\n",
        "            hidden_states = model.transformer.h[layer].output[0]\n",
        "            # Add the h-vector to the residual stream, at the last sequence position\n",
        "            hidden_states[:, -1] += h\n",
        "\n",
        "    # Get the output (token IDs), reshape it into 2 rows of (no intervention, intervention)\n",
        "    token_completions = generator.output[:, -1].reshape(2, zero_shot_size)\n",
        "    # Decode to get the string tokens\n",
        "    completions = model.tokenizer.batch_decode(token_completions[0])\n",
        "    completions_intervention = model.tokenizer.batch_decode(token_completions[1])\n",
        "\n",
        "    return zero_shot_dataset, completions, completions_intervention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e54fbd1d",
      "metadata": {},
      "source": [
        "Next, you can run the code below to see the results of your intervention. In cases where your model is correct, its completion is highlighted in green.\n",
        "\n",
        "(Note, we're using the `repr` function, because a lot of the completions are line breaks, and this helps us see them more clearly!)\n",
        "\n",
        "If you've done this correctly, you should see at least a few correct completions (~25%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "016d7d07",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_model_completions_on_h_intervention(\n",
        "    dataset: AntonymDataset,\n",
        "    completions: List[str],\n",
        "    completions_intervention: List[str],\n",
        "    num_to_display: int = 20,\n",
        ") -> None:\n",
        "    table = Table(\n",
        "        \"Prompt\", \"Model's completion\\n(no intervention)\", \"Model's completion\\n(intervention)\", \"Correct completion\",\n",
        "        title=\"Model's antonym completions\\n(green = first token is a match)\"\n",
        "    )\n",
        "\n",
        "    for i in range(min(len(completions), num_to_display)):\n",
        "\n",
        "        completion_ni = completions[i]\n",
        "        completion_i = completions_intervention[i]\n",
        "        correct_completion = dataset.completions[i]\n",
        "        correct_completion_first_token = tokenizer.tokenize(correct_completion)[0].replace('Ġ', ' ')\n",
        "        seq = dataset.seqs[i]\n",
        "        \n",
        "        # Color code the completion based on whether it's correct\n",
        "        is_correct = (completion_i == correct_completion_first_token)\n",
        "        completion_i = f\"[b green]{repr(completion_i)}[/]\" if is_correct else repr(completion_i)\n",
        "\n",
        "        table.add_row(str(seq), repr(completion_ni), completion_i, repr(correct_completion))\n",
        "\n",
        "    rprint(table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "efba280b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">         Model's antonym completions (green = first token is a match)         </span>\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">             </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\">                    </span>┃\n",
              "┃<span style=\"font-weight: bold\"> Prompt      </span>┃<span style=\"font-weight: bold\"> (no intervention)  </span>┃<span style=\"font-weight: bold\"> (intervention)     </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ awake -&gt;    │ ' awake'           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' asleep'</span>          │ ' asleep'          │\n",
              "│ first -&gt;    │ ' second'          │ ' second'          │ ' last'            │\n",
              "│ increase -&gt; │ ' increase'        │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' decrease'</span>        │ ' decrease'        │\n",
              "│ fantasy -&gt;  │ ' yes'             │ ' fantasy'         │ ' reality'         │\n",
              "│ loose -&gt;    │ ' loose'           │ ' loose'           │ ' tight'           │\n",
              "│ first -&gt;    │ ' second'          │ ' second'          │ ' last'            │\n",
              "│ paradise -&gt; │ ' paradise'        │ ' paradise'        │ ' hell'            │\n",
              "│ courage -&gt;  │ ' courage'         │ ' courage'         │ ' fear'            │\n",
              "│ future -&gt;   │ ' future'          │ ' future'          │ ' past'            │\n",
              "│ worst -&gt;    │ ' worst'           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' best'</span>            │ ' best'            │\n",
              "│ hot -&gt;      │ ' hot'             │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' cold'</span>            │ ' cold'            │\n",
              "│ reject -&gt;   │ ' reject'          │ ' reject'          │ ' accept'          │\n",
              "│ sane -&gt;     │ ' yes'             │ ' sane'            │ ' insane'          │\n",
              "│ rough -&gt;    │ ' rough'           │ ' rough'           │ ' gentle'          │\n",
              "│ moral -&gt;    │ ' moral'           │ ' moral'           │ ' immoral'         │\n",
              "│ arrive -&gt;   │ ' arrive'          │ ' arrive'          │ ' depart'          │\n",
              "│ low -&gt;      │ ' low'             │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' high'</span>            │ ' high'            │\n",
              "│ stupid -&gt;   │ ' stupid'          │ ' stupid'          │ ' smart'           │\n",
              "│ courage -&gt;  │ ' courage'         │ ' courage'         │ ' fear'            │\n",
              "│ inferior -&gt; │ ' superior'        │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' superior'</span>        │ ' superior'        │\n",
              "└─────────────┴────────────────────┴────────────────────┴────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m         Model's antonym completions (green = first token is a match)         \u001b[0m\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m             \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                    \u001b[0m┃\n",
              "┃\u001b[1m \u001b[0m\u001b[1mPrompt     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(no intervention) \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(intervention)    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ awake ->    │ ' awake'           │ \u001b[1;32m' asleep'\u001b[0m          │ ' asleep'          │\n",
              "│ first ->    │ ' second'          │ ' second'          │ ' last'            │\n",
              "│ increase -> │ ' increase'        │ \u001b[1;32m' decrease'\u001b[0m        │ ' decrease'        │\n",
              "│ fantasy ->  │ ' yes'             │ ' fantasy'         │ ' reality'         │\n",
              "│ loose ->    │ ' loose'           │ ' loose'           │ ' tight'           │\n",
              "│ first ->    │ ' second'          │ ' second'          │ ' last'            │\n",
              "│ paradise -> │ ' paradise'        │ ' paradise'        │ ' hell'            │\n",
              "│ courage ->  │ ' courage'         │ ' courage'         │ ' fear'            │\n",
              "│ future ->   │ ' future'          │ ' future'          │ ' past'            │\n",
              "│ worst ->    │ ' worst'           │ \u001b[1;32m' best'\u001b[0m            │ ' best'            │\n",
              "│ hot ->      │ ' hot'             │ \u001b[1;32m' cold'\u001b[0m            │ ' cold'            │\n",
              "│ reject ->   │ ' reject'          │ ' reject'          │ ' accept'          │\n",
              "│ sane ->     │ ' yes'             │ ' sane'            │ ' insane'          │\n",
              "│ rough ->    │ ' rough'           │ ' rough'           │ ' gentle'          │\n",
              "│ moral ->    │ ' moral'           │ ' moral'           │ ' immoral'         │\n",
              "│ arrive ->   │ ' arrive'          │ ' arrive'          │ ' depart'          │\n",
              "│ low ->      │ ' low'             │ \u001b[1;32m' high'\u001b[0m            │ ' high'            │\n",
              "│ stupid ->   │ ' stupid'          │ ' stupid'          │ ' smart'           │\n",
              "│ courage ->  │ ' courage'         │ ' courage'         │ ' fear'            │\n",
              "│ inferior -> │ ' superior'        │ \u001b[1;32m' superior'\u001b[0m        │ ' superior'        │\n",
              "└─────────────┴────────────────────┴────────────────────┴────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = AntonymDataset(WORD_PAIRS, size=20, n_prepended=2)\n",
        "\n",
        "zero_shot_dataset, model_completions, model_completions_intervention = intervene_with_h(dataset, layer=12, zero_shot_size=20)\n",
        "\n",
        "display_model_completions_on_h_intervention(zero_shot_dataset, model_completions, model_completions_intervention)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc4af4e1",
      "metadata": {},
      "source": [
        "### Exercise - combine the functions `calculate_h` and `intervene_with_h`\n",
        "\n",
        "```c\n",
        "Difficulty: 🔴🔴🔴⚪⚪\n",
        "Importance: 🔵🔵🔵⚪⚪\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n",
        "\n",
        "One great feature of the `nnsight` library is its ability to parallelize forward passes and perform complex interventions within a single context manager.\n",
        "\n",
        "In the code above, we had one function to extract the hidden states from the model, and another function where we intervened with those hidden states. But we can actually do both at once: we can compute $h$ within our forward pass, and then intervene with it on a different forward pass (using our zero-shot prompts), all within the same `model.generate` context manager. In other words, **we'll be using `with generator.invoke...` three times** in this context manager.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-3.png\" width=\"1000\">\n",
        "\n",
        "You should fill in the `calculate_h_and_intervene` function below, to do this. Mostly, this should involve combining your `calculate_h` and `intervene_with_h` functions, and wrapping the forward passes in the same context manager (plus a bit of code rewriting).\n",
        "\n",
        "Your output should be exactly the same as before (since the `AntonymDataset` class is deterministic).\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm not sure how to work with the <code>h</code> vector.</summary>\n",
        "\n",
        "You extract `h` the same way as before, but you don't need to save it, or ever reference its `.value` attribute. It is kept as a proxy. You can still use it later in the context manager, just like it actually was a tensor.\n",
        "\n",
        "You shouldn't have to `.save()` anything inside your context manager.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "194f3025",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_h_and_intervene(\n",
        "    dataset: AntonymDataset,\n",
        "    layer: int,\n",
        "    zero_shot_size: int,\n",
        ") -> Tuple[AntonymDataset, List[str]]:\n",
        "    '''\n",
        "    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,\n",
        "    all within the same forward pass.\n",
        "\n",
        "    Inputs:\n",
        "        word_list: the list of words used to create the prompts\n",
        "        dataset: the dataset of clean prompts from which we'll extract the `h`-vector\n",
        "        layer: the layer we'll be extracting the `h`-vector from\n",
        "        zero_shot_size: the number of zero-shot prompts to generate, to test our intervention\n",
        "    \n",
        "    Returns:\n",
        "        zero_shot_dataset: the dataset of zero-shot prompts, which you should generate in this fn\n",
        "        model_completions: list of string completions for the zero-shot prompts, without intervention\n",
        "        model_completions_intervention: list of string completions for the zero-shot prompts, with h-intervention\n",
        "    '''\n",
        "\n",
        "    # Get zero-shot dataset\n",
        "    zero_shot_dataset = AntonymDataset(WORD_PAIRS, size=zero_shot_size, n_prepended=0)\n",
        "\n",
        "    with model.generate(max_new_tokens=1) as generator:\n",
        "        \n",
        "        # Run on the clean prompts, to get the h-vector\n",
        "        with generator.invoke(dataset.prompts) as invoker:\n",
        "            # Define h (we don't need to save it, cause we don't need it outside `generator:`)\n",
        "            hidden_states = model.transformer.h[layer].output[0]\n",
        "            h = hidden_states[:, -1].mean(dim=0)\n",
        "\n",
        "        # First, run a forward pass where we don't intervene\n",
        "        with generator.invoke(zero_shot_dataset.prompts) as invoker:\n",
        "            pass\n",
        "\n",
        "        # Next, run a forward pass on the zero-shot prompts where we do intervene\n",
        "        with generator.invoke(zero_shot_dataset.prompts) as invoker:\n",
        "            # Access the tensor (which is the first element of the output tuple)\n",
        "            hidden_states = model.transformer.h[layer].output[0]\n",
        "            # Add the h-vector to the residual stream, at the last sequence position\n",
        "            hidden_states[:, -1] += h\n",
        "\n",
        "    # Get the output (token IDs), keep data from zero-shot dataset reshape into (no intervention, intervention)\n",
        "    token_completions = generator.output[len(dataset):, -1].reshape(2, zero_shot_size)\n",
        "    # Decode to get the string tokens\n",
        "    model_completions = model.tokenizer.batch_decode(token_completions[0])\n",
        "    model_completions_intervention = model.tokenizer.batch_decode(token_completions[1])\n",
        "\n",
        "    return zero_shot_dataset, model_completions, model_completions_intervention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ad7cb4de",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">         Model's antonym completions (green = first token is a match)         </span>\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">             </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\">                    </span>┃\n",
              "┃<span style=\"font-weight: bold\"> Prompt      </span>┃<span style=\"font-weight: bold\"> (no intervention)  </span>┃<span style=\"font-weight: bold\"> (intervention)     </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ awake -&gt;    │ ' awake'           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' asleep'</span>          │ ' asleep'          │\n",
              "│ first -&gt;    │ ' second'          │ ' second'          │ ' last'            │\n",
              "│ increase -&gt; │ ' increase'        │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' decrease'</span>        │ ' decrease'        │\n",
              "│ fantasy -&gt;  │ ' yes'             │ ' fantasy'         │ ' reality'         │\n",
              "│ loose -&gt;    │ ' loose'           │ ' loose'           │ ' tight'           │\n",
              "│ first -&gt;    │ ' second'          │ ' second'          │ ' last'            │\n",
              "│ paradise -&gt; │ ' paradise'        │ ' paradise'        │ ' hell'            │\n",
              "│ courage -&gt;  │ ' courage'         │ ' courage'         │ ' fear'            │\n",
              "│ future -&gt;   │ ' future'          │ ' future'          │ ' past'            │\n",
              "│ worst -&gt;    │ ' worst'           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' best'</span>            │ ' best'            │\n",
              "│ hot -&gt;      │ ' hot'             │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' cold'</span>            │ ' cold'            │\n",
              "│ reject -&gt;   │ ' reject'          │ ' reject'          │ ' accept'          │\n",
              "│ sane -&gt;     │ ' yes'             │ ' sane'            │ ' insane'          │\n",
              "│ rough -&gt;    │ ' rough'           │ ' rough'           │ ' gentle'          │\n",
              "│ moral -&gt;    │ ' moral'           │ ' moral'           │ ' immoral'         │\n",
              "│ arrive -&gt;   │ ' arrive'          │ ' arrive'          │ ' depart'          │\n",
              "│ low -&gt;      │ ' low'             │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' high'</span>            │ ' high'            │\n",
              "│ stupid -&gt;   │ ' stupid'          │ ' stupid'          │ ' smart'           │\n",
              "│ courage -&gt;  │ ' courage'         │ ' courage'         │ ' fear'            │\n",
              "│ inferior -&gt; │ ' superior'        │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' superior'</span>        │ ' superior'        │\n",
              "└─────────────┴────────────────────┴────────────────────┴────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m         Model's antonym completions (green = first token is a match)         \u001b[0m\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m             \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                    \u001b[0m┃\n",
              "┃\u001b[1m \u001b[0m\u001b[1mPrompt     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(no intervention) \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(intervention)    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ awake ->    │ ' awake'           │ \u001b[1;32m' asleep'\u001b[0m          │ ' asleep'          │\n",
              "│ first ->    │ ' second'          │ ' second'          │ ' last'            │\n",
              "│ increase -> │ ' increase'        │ \u001b[1;32m' decrease'\u001b[0m        │ ' decrease'        │\n",
              "│ fantasy ->  │ ' yes'             │ ' fantasy'         │ ' reality'         │\n",
              "│ loose ->    │ ' loose'           │ ' loose'           │ ' tight'           │\n",
              "│ first ->    │ ' second'          │ ' second'          │ ' last'            │\n",
              "│ paradise -> │ ' paradise'        │ ' paradise'        │ ' hell'            │\n",
              "│ courage ->  │ ' courage'         │ ' courage'         │ ' fear'            │\n",
              "│ future ->   │ ' future'          │ ' future'          │ ' past'            │\n",
              "│ worst ->    │ ' worst'           │ \u001b[1;32m' best'\u001b[0m            │ ' best'            │\n",
              "│ hot ->      │ ' hot'             │ \u001b[1;32m' cold'\u001b[0m            │ ' cold'            │\n",
              "│ reject ->   │ ' reject'          │ ' reject'          │ ' accept'          │\n",
              "│ sane ->     │ ' yes'             │ ' sane'            │ ' insane'          │\n",
              "│ rough ->    │ ' rough'           │ ' rough'           │ ' gentle'          │\n",
              "│ moral ->    │ ' moral'           │ ' moral'           │ ' immoral'         │\n",
              "│ arrive ->   │ ' arrive'          │ ' arrive'          │ ' depart'          │\n",
              "│ low ->      │ ' low'             │ \u001b[1;32m' high'\u001b[0m            │ ' high'            │\n",
              "│ stupid ->   │ ' stupid'          │ ' stupid'          │ ' smart'           │\n",
              "│ courage ->  │ ' courage'         │ ' courage'         │ ' fear'            │\n",
              "│ inferior -> │ ' superior'        │ \u001b[1;32m' superior'\u001b[0m        │ ' superior'        │\n",
              "└─────────────┴────────────────────┴────────────────────┴────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = AntonymDataset(WORD_PAIRS, size=20, n_prepended=2)\n",
        "\n",
        "zero_shot_dataset, model_completions, model_completions_intervention = calculate_h_and_intervene(dataset, layer=12, zero_shot_size=20)\n",
        "\n",
        "display_model_completions_on_h_intervention(zero_shot_dataset, model_completions, model_completions_intervention)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871b3af4",
      "metadata": {},
      "source": [
        "## Logit outputs\n",
        "\n",
        "Currently, we've only seen what the model's highest-probability output is, because that's all we got from the `generator.output` object. But what if we want to look at the logits / probabilities instead, and see how those change when we intervene?\n",
        "\n",
        "There are two ways you can get the model's logits:\n",
        "\n",
        "1. **Add more arguments to the `generate` method.**\n",
        "\n",
        "Just like standard HuggingFace models have extra arguments which can be supplied to the `generate` method, so do `nnsight` models. We can replace the line:\n",
        "\n",
        "```python\n",
        "with model.generate(...) as generator:\n",
        "```\n",
        "\n",
        "with:\n",
        "\n",
        "```python\n",
        "with model.generate(..., output_scores=True, return_dict_in_generate=True) as generator:\n",
        "```\n",
        "\n",
        "and then the `generator.output` object won't be a tensor containing the model's completion, instead it will be an object that contains both the completions ***and*** the logits. From this object, you can access:\n",
        "\n",
        "* `generator.output.sequences` = model completions, i.e. a tensor of token IDs, of shape `(batch_size, seq_len)`\n",
        "* `generator.output.scores` = logits, in the form of a tuple of tensors of shape `(batch_size, seq_len, vocab_size)` (the tuple has one element for each token generation, so in this case it will be length 1).\n",
        "\n",
        "2. **For general models, you can access the logits just like you would any other hidden state.**\n",
        "\n",
        "For example, in the case of GPT-J and other similar models, you can access the output of the final linear layer of the transformer - i.e. the one that maps from the hidden state to logits - with `model.lm_head`. You can then use the `.output` method to get a proxy for the output of this layer, and `.save()` to save it, just like you've done in previous exercises.\n",
        "\n",
        "Either of these approaches are fine, which one you use is up to personal preference. The solutions will use the first approach."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c0d643",
      "metadata": {},
      "source": [
        "### Exercise - compute change in accuracy\n",
        "\n",
        "```c\n",
        "Difficulty: 🔴🔴⚪⚪⚪\n",
        "Importance: 🔵🔵🔵⚪⚪\n",
        "\n",
        "You should spend up to 10-20 minutes on this exercise.\n",
        "```\n",
        "\n",
        "You should now rewrite the `calculate_h_and_intervene` function so that, rather than returning the string completions, it returns two lists of floats, containing the **logprobs assigned by the model to the correct antonym** in the no intervention / intervention cases respectively.\n",
        "\n",
        "When you run the code below this function, it will display the log-probabilities (highlighting green when they increase from the zero-shot case). You should find that in every sequence, the logprobs on the correct token increase in the intervention. This helps make something clear - **even if the maximum-likelihood token doesn't change, this doesn't mean that the intervention isn't having a significant effect.**\n",
        "\n",
        "<details>\n",
        "<summary>Help - I don't know how to get the correct logprobs from the logits.</summary>\n",
        "\n",
        "First, apply log softmax to the logits, to get logprobs.\n",
        "\n",
        "Second, you can use `tokenizer(dataset.completion)[\"input_ids\"]` to get the token IDs of the correct completions. (Gotcha - some words might be tokenized into multiple tokens, so make sure you're just picking the first token ID for each completion.)\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a73a3db9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_h_and_intervene_logprobs(\n",
        "    dataset: AntonymDataset,\n",
        "    layer: int,\n",
        "    zero_shot_size: int,\n",
        ") -> Tuple[AntonymDataset, List[float], List[float]]:\n",
        "    '''\n",
        "    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,\n",
        "    all within the same forward pass.\n",
        "\n",
        "    Inputs:\n",
        "        word_list: the list of words used to create the prompts\n",
        "        dataset: the dataset of clean prompts from which we'll extract the `h`-vector\n",
        "        layer: the layer we'll be extracting the `h`-vector from\n",
        "        zero_shot_size: the number of zero-shot prompts to generate, to test our intervention\n",
        "    \n",
        "    Returns:\n",
        "        zero_shot_dataset: the dataset of zero-shot prompts, which you should generate in this fn\n",
        "        correct_logprobs: list of correct-token logprobs for the zero-shot prompts, without intervention\n",
        "        correct_logprobs_intervention: list of correct-token logprobs for the zero-shot prompts, with h-intervention\n",
        "    '''\n",
        "\n",
        "    # Get zero-shot dataset\n",
        "    zero_shot_dataset = AntonymDataset(WORD_PAIRS, size=zero_shot_size, n_prepended=0)\n",
        "\n",
        "    with model.generate(max_new_tokens=1, pad_token_id=tokenizer.eos_token_id, output_scores=True, return_dict_in_generate=True) as generator:\n",
        "        \n",
        "        # Clean prompts, to get the h-vector\n",
        "        with generator.invoke(dataset.prompts) as invoker:\n",
        "            # Define h (we don't need to save it, cause we don't need it outside `generator:`)\n",
        "            hidden_states = model.transformer.h[layer].output[0]\n",
        "            h = hidden_states[:, -1].mean(dim=0)\n",
        "\n",
        "        # Zero-shot prompts, no intervention\n",
        "        with generator.invoke(zero_shot_dataset.prompts) as invoker:\n",
        "            pass\n",
        "\n",
        "        # Zero-shot prompts, intervention with h\n",
        "        with generator.invoke(zero_shot_dataset.prompts) as invoker:\n",
        "            # Access the tensor (which is the first element of the output tuple)\n",
        "            hidden_states = model.transformer.h[layer].output[0]\n",
        "            # Add the h-vector to the residual stream, at the last sequence position\n",
        "            hidden_states[:, -1] += h\n",
        "\n",
        "    # Get logits, slice to remove the `dataset` outputs, and reshape into (2, zero_shot_size, d_vocab)\n",
        "    logits: Tensor = generator.output.scores[0][len(dataset):].reshape(2, zero_shot_size, -1)\n",
        "    logprobs = logits.log_softmax(dim=-1)\n",
        "\n",
        "    # Get correct completions from `dataset`, and use these to index into the logprobs\n",
        "    correct_completion_ids = [toks[0] for toks in tokenizer(zero_shot_dataset.completions)[\"input_ids\"]]\n",
        "    correct_logprobs, correct_logprobs_intervention = logprobs[:, range(zero_shot_size), correct_completion_ids].tolist()\n",
        "\n",
        "    return zero_shot_dataset, correct_logprobs, correct_logprobs_intervention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "d09fdabb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_model_logprobs_on_h_intervention(\n",
        "    dataset: AntonymDataset,\n",
        "    correct_logprobs: List[float],\n",
        "    correct_logprobs_intervention: List[float],\n",
        "    num_to_display: int = 20,\n",
        ") -> None:\n",
        "    table = Table(\n",
        "        \"Zero-shot prompt\", \"Model's logprob\\n(no intervention)\", \"Model's logprob\\n(intervention)\", \"Change in logprob\",\n",
        "        title=\"Model's antonym logprobs, with zero-shot h-intervention\\n(green = intervention improves accuracy)\"\n",
        "    )\n",
        "\n",
        "    for i in range(min(len(correct_logprobs), num_to_display)):\n",
        "\n",
        "        logprob_ni = correct_logprobs[i]\n",
        "        logprob_i = correct_logprobs_intervention[i]\n",
        "        delta_logprob = logprob_i - logprob_ni\n",
        "        zero_shot_prompt = f\"{dataset[i].x[0]:>8} -> {dataset[i].y[0]}\"\n",
        "        \n",
        "        # Color code the logprob based on whether it's increased with this intervention\n",
        "        is_improvement = (delta_logprob >= 0)\n",
        "        delta_logprob = f\"[b green]{delta_logprob:+.2f}[/]\" if is_improvement else f\"{delta_logprob:+.2f}\"\n",
        "\n",
        "        table.add_row(zero_shot_prompt, f\"{logprob_ni:.2f}\", f\"{logprob_i:.2f}\", delta_logprob)\n",
        "\n",
        "    rprint(table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "a823d788",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">             Model's antonym logprobs, with zero-shot h-intervention              </span>\n",
              "<span style=\"font-style: italic\">                     (green = intervention improves accuracy)                     </span>\n",
              "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">                      </span>┃<span style=\"font-weight: bold\"> Model's logprob   </span>┃<span style=\"font-weight: bold\"> Model's logprob </span>┃<span style=\"font-weight: bold\">                   </span>┃\n",
              "┃<span style=\"font-weight: bold\"> Zero-shot prompt     </span>┃<span style=\"font-weight: bold\"> (no intervention) </span>┃<span style=\"font-weight: bold\"> (intervention)  </span>┃<span style=\"font-weight: bold\"> Change in logprob </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│    awake -&gt; asleep   │ -3.48             │ -1.83           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.65</span>             │\n",
              "│    first -&gt; last     │ -4.49             │ -3.04           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.45</span>             │\n",
              "│ increase -&gt; decrease │ -3.28             │ -0.97           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.30</span>             │\n",
              "│  fantasy -&gt; reality  │ -7.87             │ -4.71           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+3.16</span>             │\n",
              "│    loose -&gt; tight    │ -5.53             │ -3.20           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.33</span>             │\n",
              "│    first -&gt; last     │ -4.49             │ -3.04           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.45</span>             │\n",
              "│ paradise -&gt; hell     │ -4.99             │ -3.27           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.72</span>             │\n",
              "│  courage -&gt; fear     │ -5.21             │ -3.48           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.73</span>             │\n",
              "│   future -&gt; past     │ -3.82             │ -2.62           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.20</span>             │\n",
              "│    worst -&gt; best     │ -2.07             │ -1.34           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+0.73</span>             │\n",
              "│      hot -&gt; cold     │ -2.18             │ -0.60           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.58</span>             │\n",
              "│   reject -&gt; accept   │ -3.92             │ -2.20           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.72</span>             │\n",
              "│     sane -&gt; insane   │ -7.13             │ -4.45           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.68</span>             │\n",
              "│    rough -&gt; gentle   │ -7.49             │ -5.66           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.82</span>             │\n",
              "│    moral -&gt; immoral  │ -4.61             │ -2.98           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.63</span>             │\n",
              "│   arrive -&gt; depart   │ -5.66             │ -4.06           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.60</span>             │\n",
              "│      low -&gt; high     │ -1.96             │ -0.80           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.16</span>             │\n",
              "│   stupid -&gt; smart    │ -5.35             │ -3.52           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.83</span>             │\n",
              "│  courage -&gt; fear     │ -5.21             │ -3.48           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.73</span>             │\n",
              "│ inferior -&gt; superior │ -1.88             │ -1.02           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+0.87</span>             │\n",
              "└──────────────────────┴───────────────────┴─────────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m             Model's antonym logprobs, with zero-shot h-intervention              \u001b[0m\n",
              "\u001b[3m                     (green = intervention improves accuracy)                     \u001b[0m\n",
              "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m                      \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's logprob  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's logprob\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                   \u001b[0m┃\n",
              "┃\u001b[1m \u001b[0m\u001b[1mZero-shot prompt    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(no intervention)\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(intervention) \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mChange in logprob\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│    awake -> asleep   │ -3.48             │ -1.83           │ \u001b[1;32m+1.65\u001b[0m             │\n",
              "│    first -> last     │ -4.49             │ -3.04           │ \u001b[1;32m+1.45\u001b[0m             │\n",
              "│ increase -> decrease │ -3.28             │ -0.97           │ \u001b[1;32m+2.30\u001b[0m             │\n",
              "│  fantasy -> reality  │ -7.87             │ -4.71           │ \u001b[1;32m+3.16\u001b[0m             │\n",
              "│    loose -> tight    │ -5.53             │ -3.20           │ \u001b[1;32m+2.33\u001b[0m             │\n",
              "│    first -> last     │ -4.49             │ -3.04           │ \u001b[1;32m+1.45\u001b[0m             │\n",
              "│ paradise -> hell     │ -4.99             │ -3.27           │ \u001b[1;32m+1.72\u001b[0m             │\n",
              "│  courage -> fear     │ -5.21             │ -3.48           │ \u001b[1;32m+1.73\u001b[0m             │\n",
              "│   future -> past     │ -3.82             │ -2.62           │ \u001b[1;32m+1.20\u001b[0m             │\n",
              "│    worst -> best     │ -2.07             │ -1.34           │ \u001b[1;32m+0.73\u001b[0m             │\n",
              "│      hot -> cold     │ -2.18             │ -0.60           │ \u001b[1;32m+1.58\u001b[0m             │\n",
              "│   reject -> accept   │ -3.92             │ -2.20           │ \u001b[1;32m+1.72\u001b[0m             │\n",
              "│     sane -> insane   │ -7.13             │ -4.45           │ \u001b[1;32m+2.68\u001b[0m             │\n",
              "│    rough -> gentle   │ -7.49             │ -5.66           │ \u001b[1;32m+1.82\u001b[0m             │\n",
              "│    moral -> immoral  │ -4.61             │ -2.98           │ \u001b[1;32m+1.63\u001b[0m             │\n",
              "│   arrive -> depart   │ -5.66             │ -4.06           │ \u001b[1;32m+1.60\u001b[0m             │\n",
              "│      low -> high     │ -1.96             │ -0.80           │ \u001b[1;32m+1.16\u001b[0m             │\n",
              "│   stupid -> smart    │ -5.35             │ -3.52           │ \u001b[1;32m+1.83\u001b[0m             │\n",
              "│  courage -> fear     │ -5.21             │ -3.48           │ \u001b[1;32m+1.73\u001b[0m             │\n",
              "│ inferior -> superior │ -1.88             │ -1.02           │ \u001b[1;32m+0.87\u001b[0m             │\n",
              "└──────────────────────┴───────────────────┴─────────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = AntonymDataset(WORD_PAIRS, size=20, n_prepended=2)\n",
        "\n",
        "zero_shot_dataset, correct_logprobs, correct_logprobs_intervention = calculate_h_and_intervene_logprobs(dataset, layer=12, zero_shot_size=30)\n",
        "\n",
        "display_model_logprobs_on_h_intervention(zero_shot_dataset, correct_logprobs, correct_logprobs_intervention)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e99652a3",
      "metadata": {},
      "source": [
        "# 3️⃣ Function Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1a9deb5",
      "metadata": {},
      "source": [
        "In this section, we'll move from thinking about residual stream states to thinking about the **output of specific attention heads.**\n",
        "\n",
        "First, a bit of a technical complication. Most HuggingFace models don't have the nice attention head representations that TransformerLens models do (i.e. storing vectors & attention weights separately by heads). In the case of GPT-J, the input to `out_proj` (the final linear map in the attention layer) is a tensor of value vectors which has already been concatenated along attention heads, and applying `out_proj` is equivalent to summing over the attention heads (if you can't see how this is possible, see the section \"Attention Heads are Independent and Additive\" from Anthropic's [Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html)).\n",
        "\n",
        "How can we deal with this? The easiest way is to just intervene on the input of `out_proj` instead (since this is causally the same as intervening on the output), and making sure we reshape this input tensor so that it has a head dimension (then we can intervene more easily on a per-head basis). In other words, you should intervene on the value which we've called `z` in the diagram below.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rearrange-output.png\" width=\"950\">\n",
        "\n",
        "When you actually need to calculate `a` (the output for a particular attention head), the easiest thing to do is just apply the appropriate slice of the linear map to `z` (we'll get to this in the next exercise, so don't worry about it for now)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "729112d6",
      "metadata": {},
      "source": [
        "### Exercise - implement `calculate_fn_vectors_and_intervene`\n",
        "\n",
        "```c\n",
        "Difficulty: 🔴🔴🔴🔴⚪\n",
        "Importance: 🔵🔵🔵🔵🔵\n",
        "\n",
        "You should spend up to 30-45 minutes on this exercise.\n",
        "```\n",
        "\n",
        "This is probably the most important function in today's exercises. Implementing it will be pretty similar to the previous function `calculate_h_and_intervene`, but:\n",
        "\n",
        "* Rather than extracting the value of the residual stream `h` at some particular layer, you'll be extracting the output of the attention heads: iterating over each layer and each head in the model.\n",
        "    * You'll only need to run one clean forward pass to compute all these values, but you'll need to run a separate corrupted forward pass for each head.\n",
        "* Rather than your 2 different datasets being (dataset, zero-shot dataset), your two datasets will be (dataset, corrupted version of that same dataset).\n",
        "    * You can use the `create_corrupted_dataset` method of the `AntonymDataset` class for this.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/cie-intervention.png\" width=\"1200\">\n",
        "\n",
        "Before you actually start writing the code, it might be helpful to answer the following questions:\n",
        "\n",
        "<details>\n",
        "<summary>What will your total batch size be (if your dataset has size <code>N</code>) ?</summary>\n",
        "\n",
        "Your batch size will be `N * ((N_LAYERS * N_HEADS) + 2)`, because you'll need to run:\n",
        "\n",
        "* A fowrad pass on the clean dataset (batch size `N`),\n",
        "* A forward pass on the corrupted dataset (batch size `N`) with no intevention,\n",
        "* A forward pass on the corrupted dataset (batch size `N`) once for each head, where you're intervening on the heads.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Which proxy outputs (if any) will you need to use <code>.save()</code> on, in this function?</summary>\n",
        "\n",
        "None. You're just doing causal interventions, and getting the logits. You don't need to save the $z$-tensors in order to causally intervene with them at later points within the context manager (see the solution for the `calculate_h_and_intervene` exercises).\n",
        "\n",
        "</details>\n",
        "\n",
        "A few other tips:\n",
        "\n",
        "* When it comes to intervening, you can set the value of a reshaped tensor, i.e. `tensor.reshape(*new_shape)[index] = new_value` will change the values in `tensor` without actually reshaping it (for more on this, see the documentation for [`torch.Tensor.view`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)).\n",
        "* It's good practice to insert a lot of assert statements in your code, to check the shapes are what you expect.\n",
        "* If you're confused about dimensions, use `einops.rearrange` rather than `.reshape` - it's like using code annotations within your actual code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "fb24d120",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_fn_vectors_and_intervene(\n",
        "    dataset: AntonymDataset,\n",
        "    LAYERS = None,\n",
        ") -> Float[Tensor, \"layers heads\"]:\n",
        "    '''\n",
        "    Returns a tensor of shape (layers, heads), containing the CIE for each head.\n",
        "\n",
        "    TODO - remove the LAYERS argument (it's only necessary because of batch size constraints, which will\n",
        "    hopefully be fixed when servers work).\n",
        "\n",
        "    Inputs:\n",
        "        dataset: AntonymDataset\n",
        "            the dataset of clean prompts from which we'll extract the function vector (we'll also create a\n",
        "            corrupted version of this dataset for interventions)\n",
        "    '''\n",
        "\n",
        "    LAYERS = LAYERS if (LAYERS is not None) else range(8, 15) # range(N_LAYERS)\n",
        "    HEADS = range(N_HEADS)\n",
        "\n",
        "    # Get corrupted dataset\n",
        "    corrupted_dataset = dataset.create_corrupted_dataset()\n",
        "    N = len(dataset)\n",
        "\n",
        "    with model.generate(max_new_tokens=1, pad_token_id=tokenizer.eos_token_id, output_scores=True, return_dict_in_generate=True) as generator:\n",
        "\n",
        "        # Run a forward pass on clean prompts, where we store attention head outputs\n",
        "        a_dict = {}\n",
        "        with generator.invoke(dataset.prompts) as invoker:\n",
        "            for layer in LAYERS:\n",
        "                # Get hidden states, reshape to get head dimension, store the mean tensor\n",
        "                hidden_states = model.transformer.h[layer].attn.out_proj.input[0][:, -1]\n",
        "                a = hidden_states.reshape(N, N_HEADS, D_HEAD).mean(dim=0)\n",
        "                for head in HEADS:\n",
        "                    a_dict[(layer, head)] = a[head]\n",
        "        \n",
        "        # Run a forward pass on corrupted prompts, where we don't intervene or store activations (just so we can\n",
        "        # get the logits to compare with our intervention)\n",
        "        with generator.invoke(corrupted_dataset.prompts) as invoker:\n",
        "            pass\n",
        "\n",
        "        # For each head, run a forward pass on corrupted prompts (here we need multiple different forward passes,\n",
        "        # because we're doing different interventions each time)\n",
        "        for layer in LAYERS:\n",
        "            for head in HEADS:\n",
        "                with generator.invoke(corrupted_dataset.prompts) as invoker:\n",
        "                    # Get hidden states, reshape to get head dimension, then set it to the a-vector\n",
        "                    hidden_states = model.transformer.h[layer].attn.out_proj.input[0][:, -1]\n",
        "                    hidden_states.reshape(N, N_HEADS, D_HEAD)[:, head] = a_dict[(layer, head)]\n",
        "\n",
        "\n",
        "    # Get output logits (which contains all `n_heads+2` sub-batches of size N, concatenated) and reshape into sub-batches\n",
        "    output_logits = einops.rearrange(generator.output.scores[0], \"(batch N) d_vocab -> batch N d_vocab\", N=N)\n",
        "    assert output_logits.shape[0] == (len(HEADS) * len(LAYERS)) + 2\n",
        "\n",
        "    # Get the corrupted logits & the logits with intervention (i.e. red in the diagram). Reshape latter to get head dim\n",
        "    logits_corrupted = output_logits[1]\n",
        "    logits_intervention = einops.rearrange(output_logits[2:], \"(layer head) N d_vocab -> layer head N d_vocab\", head=N_HEADS)\n",
        "    \n",
        "    # Get logprobs, for correct tokens\n",
        "    correct_completion_ids = [toks[0] for toks in tokenizer(dataset.completions)[\"input_ids\"]]\n",
        "    logprobs_corrupted = logits_corrupted.log_softmax(dim=-1)[range(N), correct_completion_ids]\n",
        "    logprobs_intervention = logits_intervention.log_softmax(dim=-1)[:, :, range(N), correct_completion_ids]\n",
        "    \n",
        "    # Return mean effect of intervention, over the batch dimension\n",
        "    return (logprobs_intervention - logprobs_corrupted).mean(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2957f70",
      "metadata": {},
      "source": [
        "Need to run this in batches, for CUDA reasons (doing a forward pass with 5 prompts for each of the 28*16 heads in GPT-J is big!). I've attached the image, so I don't have to run this again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "e6705c89",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 40 seconds for 2 layers, N=6, prepend=5\n",
        "# # 85 seconds for 10 layers, N=6, prepend=5\n",
        "# # (165 -> CUDA error) seconds for 28 layers, N=6, prepend=5\n",
        "\n",
        "# results = []\n",
        "\n",
        "# dataset = AntonymDataset(WORD_PAIRS, size=6, n_prepended=3)\n",
        "\n",
        "# for LAYERS in tqdm([range(10), range(10, 20), range(20, 28)]):\n",
        "    \n",
        "#     gc.collect()\n",
        "#     t.cuda.empty_cache()\n",
        "\n",
        "#     _results = calculate_fn_vectors_and_intervene(\n",
        "#         dataset = dataset,\n",
        "#         LAYERS = LAYERS,\n",
        "#     )\n",
        "#     results.append(_results)\n",
        "\n",
        "#     gc.collect()\n",
        "#     t.cuda.empty_cache()\n",
        "    \n",
        "\n",
        "# results = t.concat(results)\n",
        "\n",
        "# imshow(\n",
        "#     results.T,\n",
        "#     title = \"Average indirect effect of function-vector intervention on antonym task\",\n",
        "#     width = 1000,\n",
        "#     height = 600,\n",
        "#     labels = {\"x\": \"Layer\", \"y\": \"Head\"},\n",
        "#     aspect = \"equal\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77157855",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/main-result.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cedb0f13",
      "metadata": {},
      "source": [
        "### Exercise - calculate the function vector\n",
        "\n",
        "```c\n",
        "Difficulty: 🔴🔴🔴⚪⚪\n",
        "Importance: 🔵🔵🔵⚪⚪\n",
        "\n",
        "You should spend up to 20-35 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Your next task is to actually calculate and return the function vector, so we can do a few experiments with it.\n",
        "\n",
        "You should pick the 10 highest-scoring attention heads (according to the diagram above). You can hardcode these.\n",
        "\n",
        "Mostly, this will involve taking your previous code and removing parts of it (since you only need to return the function vectors, not intervene with them). However, there is one difficulty here - in the previous exercises we causally intervened on the `out_proj` input (because that was easier), but here we need the actual attention head outputs. Can you see how to do this?\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "Once we have the value vectors post-attention (let's call them `z`), we can just apply the linear map corresponding to a slice of the `out_proj` weight matrix.\n",
        " \n",
        "To be more specific:\n",
        "\n",
        "* The weight matrices of linear layers are stored as `(out_features, in_features)`, meaning `out_proj.weight.shape = (d_model, d_model = n_heads * d_head)` (i.e. the first dimension is the output space of the attention heads, and the second dimension is the input space for each of the `z`-vectors, concatenated).\n",
        "* We can rearrange this along the second dimension, then index into it to get a tensor of shape `(d_model, d_head)` corresponding to a particular head.\n",
        "* Then we can take this matrix `W_O`, and calculate `W_O @ z` - this will give us the attention head's output.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "09526600",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_fn_vector(\n",
        "    dataset: AntonymDataset,\n",
        "    head_list: List[Tuple[int, int]],\n",
        ") -> Float[Tensor, \"d_model\"]:\n",
        "    '''\n",
        "    Returns a tensor of shape (layers, heads), containing the CIE for each head.\n",
        "\n",
        "    Inputs:\n",
        "        dataset: AntonymDataset\n",
        "            the dataset of clean prompts from which we'll extract the function vector (we'll also create a\n",
        "            corrupted version of this dataset for interventions)\n",
        "        head_list: List[Tuple[int, int]]\n",
        "            list of attention heads we're calculating the function vector from\n",
        "    '''\n",
        "    # Turn head_list into a dict of {layer: list_of_heads}\n",
        "    head_dict = {}\n",
        "    for layer, head in head_list:\n",
        "        head_dict[layer] = head_dict.get(layer, []) + [head]\n",
        "\n",
        "    z_dict = {}\n",
        "\n",
        "    with model.generate(max_new_tokens=1, pad_token_id=tokenizer.eos_token_id, output_scores=True, return_dict_in_generate=True) as generator:\n",
        "\n",
        "        # output_list = []\n",
        "\n",
        "        with generator.invoke(dataset.prompts) as invoker:\n",
        "            for layer, head_list in head_dict.items():\n",
        "\n",
        "                # Get the output projection layer\n",
        "                out_proj = model.transformer.h[layer].attn.out_proj\n",
        "                \n",
        "                # Get the hidden states, and the mix of value vectors (which we'll call z)\n",
        "                hidden_states = out_proj.input[0][:, -1]\n",
        "                z = hidden_states.reshape(len(dataset), N_HEADS, D_HEAD).mean(dim=0)\n",
        "\n",
        "                # For each head, compute the function vector, and add it to the list\n",
        "                for head in head_list:\n",
        "                    z_dict[(layer, head)] = z[head].save()\n",
        "\n",
        "        # fn_vector = t.stack(output_list).sum(dim=0).save()\n",
        "\n",
        "    attn_head_output_dict = {}\n",
        "    \n",
        "    for (layer, head), z in z_dict.items():\n",
        "        W_O = model.local_model.transformer.h[layer].attn.out_proj.weight.data\n",
        "        output = W_O.reshape(D_MODEL, N_HEADS, D_HEAD)[:, head] @ z.value\n",
        "        attn_head_output_dict[(layer, head)] = output\n",
        "\n",
        "    fn_vector = t.stack(list(attn_head_output_dict.values())).sum(dim=0)\n",
        "    assert fn_vector.shape == (D_MODEL,)\n",
        "\n",
        "    return fn_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d51b49",
      "metadata": {},
      "outputs": [],
      "source": [
        "tests.test_calculate_fn_vector(calculate_fn_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5ed78a8",
      "metadata": {},
      "source": [
        "## Multi-token generation\n",
        "\n",
        "We're now going to replicate some of the results in Table 3, in the paper:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/tab3.png\" width=\"700\">\n",
        "\n",
        "This will involve doing something we haven't done before - **intervening on multi-token prompt generation**.\n",
        "\n",
        "Most of the interpretability exercises in this chapter have just consisted of running single forward passes, rather than autoregressive text generation. But we're trying something different here: we're adding the function vector to the final sequence position at each forward pass during text generation, and seeing if we can get the model to output a sentence with a different meaning.\n",
        "\n",
        "The results of Table 3 came from adding the function vector to the residual stream at the final sequence position of the original prompt, **and the final sequence position for each subsequent generation.** The reason we do this is to guide the model's behaviour over time. Our hypothesis is that the function vector induces \"next-token antonym behaviour\" (because it was calculated by averaging attention head outputs at the sequence position before the model made its antonym prediction in the ICL prompts).\n",
        "\n",
        "### Using `nnsight` for multi-token generation\n",
        "\n",
        "Previously, our context managers have looked like:\n",
        "\n",
        "```python\n",
        "with model.generate(max_new_tokens=1) as generator:\n",
        "    with generator.invoke(prompt) as invoker:\n",
        "\n",
        "        # Do stuff to the model's internals\n",
        "```\n",
        "\n",
        "But for multi-token generation, our context mnagers will look like:\n",
        "\n",
        "```python\n",
        "with model.generate(max_new_tokens=max_new_tokens) as generator:\n",
        "    with generator.invoke(prompt) as invoker:\n",
        "\n",
        "        for n in range(max_new_tokens):\n",
        "            # Do stuff to the model's internals, on the n-th forward pass\n",
        "            invoker.next()\n",
        "```\n",
        "\n",
        "The line `invoker.next()` denotes that the following interventions should be applied to the subsequent generations.\n",
        "\n",
        "Mostly, the stuff you'll be used to from single-token generation generalizes to th multi-token case. The object `generator.output` is still a tensor which contains the model's token ID completions (or we can return scores instead, exactly like we did before). Using `.save()` still saves proxies outside the context managers (although make sure that you don't use the same variable names over different generations, otherwise you'll overwrite them - it's easier to store your saved proxies in e.g. a list or dict).\n",
        "\n",
        "A couple more notes:\n",
        "\n",
        "* By default the `generate` method will generate tokens greedily, i.e. always taking the maximum-probability token at each step. For now, we don't need to worry about changing this behaviour.\n",
        "* Transformer models perform **key-value caching** to speed up text generation. This means that the time taken to generate $n$ tokens is ***much*** less than $n$ times longer than generating a single token. See [this blog post](https://kipp.ly/transformer-inference-arithmetic/) on transformer inference arithmetic for more."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b984451",
      "metadata": {},
      "source": [
        "### Exercise - intervene with function vector, in multi-token generation\n",
        "\n",
        "```c\n",
        "Difficulty: 🔴🔴🔴⚪⚪\n",
        "Importance: 🔵🔵🔵🔵⚪\n",
        "\n",
        "You should spend up to 10-20 minutes on this exercise.\n",
        "```\n",
        "\n",
        "You should now fill in the function `intervene_with_fn_vector` below. This will take a function vector (calculated from the function you wrote above), as well as a few other arguments (see docstring), and return the model's string completion on the given prompt template. \n",
        "\n",
        "We hope to observe results qualitatively like the ones in Table 3, i.e. having the model define a particular word as its antonym."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d21cbe74",
      "metadata": {},
      "outputs": [],
      "source": [
        "def intervene_with_fn_vector(\n",
        "    word: str,\n",
        "    layer: int,\n",
        "    fn_vector: Float[Tensor, \"d_model\"],\n",
        "    prompt_template = 'The word \"{x}\" means',\n",
        "    n_tokens: int = 5,\n",
        ") -> Tuple[str, str]:\n",
        "    '''\n",
        "    Intervenes with a function vector, by adding it at the last sequence position of a generated prompt.\n",
        "\n",
        "    Returns: the full completion (including original prompt) for no-intervention / intervention case respectively.\n",
        "    '''\n",
        "\n",
        "    prompt = prompt_template.format(x=word)\n",
        "    \n",
        "    with model.generate(max_new_tokens=n_tokens, pad_token_id=tokenizer.eos_token_id) as generator:\n",
        "\n",
        "        # No intervention\n",
        "        with generator.invoke(prompt) as invoker:\n",
        "            pass\n",
        "\n",
        "        # Intervention\n",
        "        with generator.invoke(prompt) as invoker:\n",
        "            for i in range(n_tokens):\n",
        "                hidden_states = model.transformer.h[layer].output[0]\n",
        "                hidden_states[:, -1] += fn_vector\n",
        "                # TODO - add exercises about invoker.next()!\n",
        "                invoker.next()\n",
        "\n",
        "    output = generator.output\n",
        "\n",
        "    completions = tokenizer.batch_decode(output)\n",
        "\n",
        "    return tuple(completions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9322f62",
      "metadata": {},
      "source": [
        "To test your function, run this code. You should find that the first completion seems normal, but the second completion defines a word as its antonym. **You've just successfully induced an OOD behavioural change in a 6b-parameter model!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "ce0fbac0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'The word \"expensive\" means different things to different people. For some, it'\n",
            "'The word \"expensive\" means \"cheap\" in this case.\\n\\n'\n"
          ]
        }
      ],
      "source": [
        "# Define our dataset, and the attention heads we'll use\n",
        "dataset = AntonymDataset(WORD_PAIRS, size=20, n_prepended=5)\n",
        "head_list = [(8, 0), (8, 1), (9, 14), (11, 0), (12, 10), (13, 12), (13, 13), (14, 9), (15, 5), (16, 14)]\n",
        "\n",
        "# Extract the function vector\n",
        "fn_vector = calculate_fn_vector(dataset, head_list)\n",
        "\n",
        "# Define a word we'll use in our prompt, and check it's OOD\n",
        "word = \"expensive\"\n",
        "assert all(word not in pair for pair in WORD_PAIRS)\n",
        "\n",
        "# Intervene with the function vector\n",
        "completion, completion_intervention = intervene_with_fn_vector(\n",
        "    word = word,\n",
        "    layer = 9,\n",
        "    fn_vector = fn_vector,\n",
        "    prompt_template = 'The word \"{x}\" means',\n",
        "    n_tokens = 10,\n",
        ")\n",
        "print(repr(completion))\n",
        "print(repr(completion_intervention))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41550988",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note to self - won't have ppl replicate results like Table 2, because this just seems kinda boring compared to the stuff we've done so far, and it doesn't introduce any new conceptual ideas. We've already done (zero shot, h-vector) and (shuffled, fn-vector). We'll leave as a bonus exercise replicating these results with other prompt templates and other models.\n",
        "\n",
        "# TODO - change the examples in the `generate_dataset` function, so they have a prepended space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41550988",
      "metadata": {},
      "source": [
        "### Exercise - generalize results to another task (optional)\n",
        "\n",
        "```c\n",
        "Difficulty: 🔴🔴🔴🔴⚪\n",
        "Importance: 🔵🔵🔵⚪⚪\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n",
        "\n",
        "In this exercise, you get to pick a task different to the antonyms task, and see if the results still hold up (for the same set of attention heads).\n",
        "\n",
        "We'll leave this exercise fairly open-ended, without any code templates for you to fill in. However, if you'd like some guidance you can use the dropdown below.\n",
        "\n",
        "<details>\n",
        "<summary>Guidance for exercise</summary>\n",
        "\n",
        "Whatever your task, you'll want to generate a new set of words. You can repurpose the `generate_dataset` function from the antonyms task, by supplying a different prompt and initial set of examples (this will require generating & using an OpenAI api key, if you haven't already), or you can just find an appropriate dataset online.\n",
        "\n",
        "When you define the `AntonymDataset`, you might want to use `bidirectional=False`, if your task isn't symmetric. The antonym task is symmetric, but others (e.g. the Country-Capitals task) are not.\n",
        "\n",
        "You'll need to supply a new prompt template for the `intervene_with_fn_vector` function, but otherwise most of your code should stay the same.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "599c6370",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished in 35.82 seconds.\n"
          ]
        }
      ],
      "source": [
        "def generate_dataset_capitals(N: int):\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    examples = \"Portugal: Lisbon, Ireland: Dublin, Chile: Santiago, Japan: Tokyo, \"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Give me {N} examples of Country-Capital pairs.\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"Sure! Here are {N} Country-Capital pairs: {examples}\"},\n",
        "        ]\n",
        "    )\n",
        "    response_text: str = examples + response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    word_pairs = [word_pair.split(\": \") for word_pair in response_text.strip(\".\\n\").split(\", \")]\n",
        "\n",
        "    print(f\"Finished in {time.time()-t0:.2f} seconds.\")\n",
        "\n",
        "    return word_pairs\n",
        "\n",
        "\n",
        "CC_WORD_PAIRS = generate_dataset_capitals(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "c0cde258",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'When you think of Netherlands, you probably think of tulips, windmills, and cheese. But the Netherlands is also home'\n",
            "'When you think of Netherlands, you probably think of tulips, windmills, and cheese. But Amsterdam is a lot more'\n"
          ]
        }
      ],
      "source": [
        "# Remove (Netherlands, Amsterdam) from the pairs, so it can be a holdout\n",
        "country = \"Netherlands\"\n",
        "_CC_WORD_PAIRS = [pair for pair in CC_WORD_PAIRS if pair[0] != country]\n",
        "\n",
        "# Define our dataset, and the attention heads we'll use\n",
        "dataset = AntonymDataset(_CC_WORD_PAIRS, size=20, n_prepended=5, bidirectional=False)\n",
        "head_list = [(8, 0), (8, 1), (9, 14), (11, 0), (12, 10), (13, 12), (13, 13), (14, 9), (15, 5), (16, 14)]\n",
        "\n",
        "# Extract the function vector\n",
        "fn_vector = calculate_fn_vector(dataset, head_list)\n",
        "\n",
        "# Intervene with the function vector\n",
        "completion, completion_intervention = intervene_with_fn_vector(\n",
        "    word = country,\n",
        "    layer = 9,\n",
        "    fn_vector = fn_vector,\n",
        "    prompt_template = 'When you think of {x},',\n",
        "    n_tokens = 20,\n",
        ")\n",
        "print(repr(completion))\n",
        "print(repr(completion_intervention))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54894689",
      "metadata": {},
      "source": [
        "# 3️⃣ Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29a2dfbe",
      "metadata": {},
      "source": [
        "There are two other interesting results from the paper, although neither of them are as important as the ones we've covered so far. If you have time, you can try to reproduce these results yourself.\n",
        "\n",
        "### 3.2 - The Decoded Vocabulary of Function Vectors\n",
        "\n",
        "In this section, the authors find the top words in the decoded vocabulary of the function vector (i.e. the words whose unembedding vectors have the highest dot product with the function vector), and show that these words seem conceptually related to the task. For example:\n",
        "\n",
        "* For the antonyms task, the top words evoke the idea of antonyms, e.g. `\" negate\"`, `\" counterpart\"`, `\" lesser\"`.\n",
        "* For the country-capitals task, the top words are actually the names of capitals, e.g. `\" Moscow\"`, `\" Paris\"`, `\" Madrid\"`.\n",
        "\n",
        "Can you replicate these results, both with the antonyms task and with the task you chose in the previous section?\n",
        "\n",
        "An interesting extension - what happens if you take a task like the Country-Capitals task (which is inherently asymmetric), and get your function vector from the symmetric version of the task (i.e. the one where each of your question-answer pairs might be flipped around)? Do you still get the same behavioural results, and how (if at all) do the decoded vocabulary results change?\n",
        "\n",
        "<details>\n",
        "<summary>My results for this (spoiler!)</summary>\n",
        "\n",
        "In the Country-Capitals task, I found:\n",
        "\n",
        "* The bidirectional task does still work to induce behavioural changes, although slightly less effectively than for the original task.\n",
        "* The top decoded vocabulary items are a mix of country names and capital names, but mostly capitals.\n",
        "\n",
        "</details>\n",
        "\n",
        "### 3.3 - Vector Algebra on Function Vectors\n",
        "\n",
        "In this section, the authors investigate whether function vectors can be composed. For instance, if we have three separate ICL tasks which in some sense compose to make a fourth task, can we add together the three function vectors of the first tasks, and use this as the function vector of the fourth task?\n",
        "\n",
        "The authors test this on a variety of different tasks. They find that it's effective on some tasks (e.g. Country-Capitals, where it outperforms function vectors), but generally isn't as effective as function vectors. Do you get these same results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b8b716",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code to calculate decoded vocabulary:\n",
        "logits = model.local_model.lm_head(fn_vector)\n",
        "max_logits = logits.topk(20).indices.tolist()\n",
        "tokens = model.tokenizer.batch_decode(max_logits)\n",
        "print(tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "071699bfa16946eb86e71be1c771994c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09b80929a3394f54b81508b9c37c35aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09e2efcbec7f4380bcb8a723c2903fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c51703652ac4b71963036085b1cd670",
              "IPY_MODEL_a20ec8ef68e64304b075c959aba148d3",
              "IPY_MODEL_0dcdafcd7b02409f8dd6a25c3f6ad429"
            ],
            "layout": "IPY_MODEL_3c710763737f4d55abdf6dc6076fc145"
          }
        },
        "0a4534c5ac36465693f20519b5b5242b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8829ee2a16d4be786b8567e5c383162",
            "placeholder": "​",
            "style": "IPY_MODEL_aa098b3568084852aea1d0f01b7ab8b5",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 10.2MB/s]"
          }
        },
        "0ad2f46e2f784d6f94bada2998a6f286": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dcdafcd7b02409f8dd6a25c3f6ad429": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dcd24e456bd4d5184ff9bd58f706e3d",
            "placeholder": "​",
            "style": "IPY_MODEL_aa922eeae3844f22bb62c11ce7aef259",
            "value": " 124/124 [00:00&lt;00:00, 3.43kB/s]"
          }
        },
        "102265d92d5e45008115e4846eb28ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ad2f46e2f784d6f94bada2998a6f286",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b89db8b54b14b82b16397bdc054a1ff",
            "value": 1042301
          }
        },
        "1a5814bbd1c746d5bdc90c01a6945025": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "206bf2c3a1ec40b9abf117adf065ed23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25a632400af84b1fb9db070680841e25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26735e27c5b147a38179b75a4f2afc99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b1d08a9778b463281c213615e70ddea",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd4246c91a394f7d9291dbd566e7d141",
            "value": 548105171
          }
        },
        "27ae3a78f39942538d30651ff528f409": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1d2cc1ef03341df84b976852fe80314",
            "placeholder": "​",
            "style": "IPY_MODEL_6f3fbc11f8d54e4fbeff7d26378d4945",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 3.94MB/s]"
          }
        },
        "286c056e9ece4d95bf45be50b126b3a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c3135fb320741e394174795fdd7be09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "376425167b2d4e04ab73d457a237fbe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b97e8c434cc41f2a6f7aef7b3e227a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c710763737f4d55abdf6dc6076fc145": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "447cc003bbde45deb12d296abf083235": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45abaef9e2ef4a349f70ecb7ffbf7ee0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47079cb5f3db45138a60b3ceea770ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b89db8b54b14b82b16397bdc054a1ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c7b40ee0c2b4f288350a6c3995da853": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3227fb65e3248d496ab03d29db1ef42",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fe79251a2144b95919ce5333708f1e5",
            "value": 1355256
          }
        },
        "4f0c16a172964b2f9efbc7a24125c113": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53b162a638d142e2a5614e36a64d7e15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c2e7e6f435b4783ab469679b4f91c05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fe79251a2144b95919ce5333708f1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6736a62a60f6400883eb1a457dc76cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a7d027471514caca29d1b0c051052c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_860886f6c58046a58c221284dc9cacad",
            "placeholder": "​",
            "style": "IPY_MODEL_fbeedd356efe4ec9b7275a04c44e4d1c",
            "value": "(…)gingface.co/gpt2/resolve/main/vocab.json: 100%"
          }
        },
        "6b574e162581400c84cde54b7cb63851": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c51703652ac4b71963036085b1cd670": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b574e162581400c84cde54b7cb63851",
            "placeholder": "​",
            "style": "IPY_MODEL_aa8947cb26ee443cb2ffb392ea6990a7",
            "value": "(…)gpt2/resolve/main/generation_config.json: 100%"
          }
        },
        "6f3fbc11f8d54e4fbeff7d26378d4945": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7847016aa41d4d9993dd2d0102f237a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a0f3516bfe7462cb48e64962e8155be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85a3b3004e3f48fbaeb6eea02892f9ad",
              "IPY_MODEL_4c7b40ee0c2b4f288350a6c3995da853",
              "IPY_MODEL_0a4534c5ac36465693f20519b5b5242b"
            ],
            "layout": "IPY_MODEL_7847016aa41d4d9993dd2d0102f237a3"
          }
        },
        "7aefe139589d44359faa8ef0e87ec690": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b1d08a9778b463281c213615e70ddea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d25b574801a4f86b1f0bc04f5259500": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f52490174cf47f881556b71209b140d",
            "placeholder": "​",
            "style": "IPY_MODEL_376425167b2d4e04ab73d457a237fbe9",
            "value": " 456k/456k [00:00&lt;00:00, 5.12MB/s]"
          }
        },
        "85a3b3004e3f48fbaeb6eea02892f9ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9b78b61315d46d48ce1f8c12778ef8f",
            "placeholder": "​",
            "style": "IPY_MODEL_6736a62a60f6400883eb1a457dc76cba",
            "value": "(…)face.co/gpt2/resolve/main/tokenizer.json: 100%"
          }
        },
        "860886f6c58046a58c221284dc9cacad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b0f8ed2f3814fad9813554a6362ffd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3aaeddb7ebc495893f72abcfdaa67bf",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25a632400af84b1fb9db070680841e25",
            "value": 456318
          }
        },
        "8d2a981b3d5342eaa0358fd71bbb0a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9e709d511d04db68b8296b74b60aeb9",
              "IPY_MODEL_26735e27c5b147a38179b75a4f2afc99",
              "IPY_MODEL_b53d7fbf9fa74958b0047c296b444255"
            ],
            "layout": "IPY_MODEL_447cc003bbde45deb12d296abf083235"
          }
        },
        "8d779276e1cf487ea5547d999d8e930c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4bd4a6d6a7a413f8990769794f1f18e",
              "IPY_MODEL_8b0f8ed2f3814fad9813554a6362ffd1",
              "IPY_MODEL_7d25b574801a4f86b1f0bc04f5259500"
            ],
            "layout": "IPY_MODEL_7aefe139589d44359faa8ef0e87ec690"
          }
        },
        "8dcd24e456bd4d5184ff9bd58f706e3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f52490174cf47f881556b71209b140d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fbd97448abf402f984e546fba008e70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9174d51e54e14f0f88cfbaab73daf389": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d240fa4a105d4f0e8e157bae02015891",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_206bf2c3a1ec40b9abf117adf065ed23",
            "value": 665
          }
        },
        "95a1e7fd51544e88b22305146c46dd09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a20ec8ef68e64304b075c959aba148d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53b162a638d142e2a5614e36a64d7e15",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_286c056e9ece4d95bf45be50b126b3a0",
            "value": 124
          }
        },
        "a43c3d5db23442fdbd1ed538b249fe40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a7d027471514caca29d1b0c051052c8",
              "IPY_MODEL_102265d92d5e45008115e4846eb28ea8",
              "IPY_MODEL_27ae3a78f39942538d30651ff528f409"
            ],
            "layout": "IPY_MODEL_4f0c16a172964b2f9efbc7a24125c113"
          }
        },
        "a4bd4a6d6a7a413f8990769794f1f18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a5814bbd1c746d5bdc90c01a6945025",
            "placeholder": "​",
            "style": "IPY_MODEL_09b80929a3394f54b81508b9c37c35aa",
            "value": "(…)gingface.co/gpt2/resolve/main/merges.txt: 100%"
          }
        },
        "a8829ee2a16d4be786b8567e5c383162": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9e709d511d04db68b8296b74b60aeb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45abaef9e2ef4a349f70ecb7ffbf7ee0",
            "placeholder": "​",
            "style": "IPY_MODEL_071699bfa16946eb86e71be1c771994c",
            "value": "model.safetensors: 100%"
          }
        },
        "aa098b3568084852aea1d0f01b7ab8b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa8947cb26ee443cb2ffb392ea6990a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa922eeae3844f22bb62c11ce7aef259": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b53d7fbf9fa74958b0047c296b444255": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c3135fb320741e394174795fdd7be09",
            "placeholder": "​",
            "style": "IPY_MODEL_fee98742d02f4995b4f34923fea412aa",
            "value": " 548M/548M [00:07&lt;00:00, 86.7MB/s]"
          }
        },
        "c84d6f13cc0a4202abfcdae55ad42dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fbd97448abf402f984e546fba008e70",
            "placeholder": "​",
            "style": "IPY_MODEL_47079cb5f3db45138a60b3ceea770ae1",
            "value": " 665/665 [00:00&lt;00:00, 32.5kB/s]"
          }
        },
        "cd4246c91a394f7d9291dbd566e7d141": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d240fa4a105d4f0e8e157bae02015891": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3aaeddb7ebc495893f72abcfdaa67bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d53a24328f7e46cfa28a16a141bfdc58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2bfd14606654735981fa6584863d21d",
              "IPY_MODEL_9174d51e54e14f0f88cfbaab73daf389",
              "IPY_MODEL_c84d6f13cc0a4202abfcdae55ad42dd2"
            ],
            "layout": "IPY_MODEL_5c2e7e6f435b4783ab469679b4f91c05"
          }
        },
        "d9b78b61315d46d48ce1f8c12778ef8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d2cc1ef03341df84b976852fe80314": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2bfd14606654735981fa6584863d21d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b97e8c434cc41f2a6f7aef7b3e227a0",
            "placeholder": "​",
            "style": "IPY_MODEL_95a1e7fd51544e88b22305146c46dd09",
            "value": "(…)ingface.co/gpt2/resolve/main/config.json: 100%"
          }
        },
        "e3227fb65e3248d496ab03d29db1ef42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbeedd356efe4ec9b7275a04c44e4d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fee98742d02f4995b4f34923fea412aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
